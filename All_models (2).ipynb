{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iP6dMIuYKJNE",
        "outputId": "7ed98321-0492-4608-9298-ca76ce1ea0ed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Transformer-based-solutions-for-the-long-term-time-series-forecasting'...\n",
            "remote: Enumerating objects: 332, done.\u001b[K\n",
            "remote: Counting objects: 100% (25/25), done.\u001b[K\n",
            "remote: Compressing objects: 100% (20/20), done.\u001b[K\n",
            "remote: Total 332 (delta 8), reused 18 (delta 5), pack-reused 307 (from 1)\u001b[K\n",
            "Receiving objects: 100% (332/332), 135.62 MiB | 14.89 MiB/s, done.\n",
            "Resolving deltas: 100% (152/152), done.\n",
            "/content/Transformer-based-solutions-for-the-long-term-time-series-forecasting\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/debi2023-group3/Transformer-based-solutions-for-the-long-term-time-series-forecasting.git\n",
        "\n",
        "\n",
        "%cd Transformer-based-solutions-for-the-long-term-time-series-forecasting"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "WAVKTmmgKP29",
        "outputId": "b5f3bf01-40c7-46c1-a63d-b1079c3a3021"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting einops==0.6.1 (from -r requirements.txt (line 1))\n",
            "  Downloading einops-0.6.1-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting numpy==1.23.5 (from -r requirements.txt (line 2))\n",
            "  Downloading numpy-1.23.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.3 kB)\n",
            "Collecting pandas==2.0.1 (from -r requirements.txt (line 3))\n",
            "  Downloading pandas-2.0.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\n",
            "Collecting scikit_learn==1.2.2 (from -r requirements.txt (line 4))\n",
            "  Downloading scikit_learn-1.2.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
            "Collecting scipy==1.9.1 (from -r requirements.txt (line 5))\n",
            "  Downloading scipy-1.9.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.2 kB)\n",
            "Collecting torchscan==0.1.2 (from -r requirements.txt (line 6))\n",
            "  Downloading torchscan-0.1.2-py3-none-any.whl.metadata (26 kB)\n",
            "Collecting tqdm==4.65.0 (from -r requirements.txt (line 7))\n",
            "  Downloading tqdm-4.65.0-py3-none-any.whl.metadata (56 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.0/57.0 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting opendatasets==0.1.22 (from -r requirements.txt (line 8))\n",
            "  Downloading opendatasets-0.1.22-py3-none-any.whl.metadata (9.2 kB)\n",
            "Collecting torch==1.13.1 (from -r requirements.txt (line 9))\n",
            "  Downloading torch-1.13.1-cp310-cp310-manylinux1_x86_64.whl.metadata (24 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas==2.0.1->-r requirements.txt (line 3)) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas==2.0.1->-r requirements.txt (line 3)) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas==2.0.1->-r requirements.txt (line 3)) (2024.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit_learn==1.2.2->-r requirements.txt (line 4)) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit_learn==1.2.2->-r requirements.txt (line 4)) (3.5.0)\n",
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.10/dist-packages (from opendatasets==0.1.22->-r requirements.txt (line 8)) (1.6.17)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from opendatasets==0.1.22->-r requirements.txt (line 8)) (8.1.7)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==1.13.1->-r requirements.txt (line 9)) (4.12.2)\n",
            "Collecting nvidia-cuda-runtime-cu11==11.7.99 (from torch==1.13.1->-r requirements.txt (line 9))\n",
            "  Downloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu11==8.5.0.96 (from torch==1.13.1->-r requirements.txt (line 9))\n",
            "  Downloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu11==11.10.3.66 (from torch==1.13.1->-r requirements.txt (line 9))\n",
            "  Downloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cuda-nvrtc-cu11==11.7.99 (from torch==1.13.1->-r requirements.txt (line 9))\n",
            "  Downloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==1.13.1->-r requirements.txt (line 9)) (71.0.4)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==1.13.1->-r requirements.txt (line 9)) (0.44.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas==2.0.1->-r requirements.txt (line 3)) (1.16.0)\n",
            "Requirement already satisfied: certifi>=2023.7.22 in /usr/local/lib/python3.10/dist-packages (from kaggle->opendatasets==0.1.22->-r requirements.txt (line 8)) (2024.7.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from kaggle->opendatasets==0.1.22->-r requirements.txt (line 8)) (2.32.3)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.10/dist-packages (from kaggle->opendatasets==0.1.22->-r requirements.txt (line 8)) (8.0.4)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.10/dist-packages (from kaggle->opendatasets==0.1.22->-r requirements.txt (line 8)) (2.0.7)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from kaggle->opendatasets==0.1.22->-r requirements.txt (line 8)) (6.1.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach->kaggle->opendatasets==0.1.22->-r requirements.txt (line 8)) (0.5.1)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.10/dist-packages (from python-slugify->kaggle->opendatasets==0.1.22->-r requirements.txt (line 8)) (1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle->opendatasets==0.1.22->-r requirements.txt (line 8)) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle->opendatasets==0.1.22->-r requirements.txt (line 8)) (3.7)\n",
            "Downloading einops-0.6.1-py3-none-any.whl (42 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.23.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.1/17.1 MB\u001b[0m \u001b[31m58.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pandas-2.0.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.3/12.3 MB\u001b[0m \u001b[31m111.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scikit_learn-1.2.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (9.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.6/9.6 MB\u001b[0m \u001b[31m117.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scipy-1.9.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (43.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.9/43.9 MB\u001b[0m \u001b[31m39.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchscan-0.1.2-py3-none-any.whl (30 kB)\n",
            "Downloading tqdm-4.65.0-py3-none-any.whl (77 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.1/77.1 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opendatasets-0.1.22-py3-none-any.whl (15 kB)\n",
            "Downloading torch-1.13.1-cp310-cp310-manylinux1_x86_64.whl (887.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m887.5/887.5 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.1/317.1 MB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.0/21.0 MB\u001b[0m \u001b[31m77.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m849.3/849.3 kB\u001b[0m \u001b[31m51.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m557.1/557.1 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tqdm, nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cublas-cu11, numpy, einops, scipy, pandas, nvidia-cudnn-cu11, torch, scikit_learn, opendatasets, torchscan\n",
            "  Attempting uninstall: tqdm\n",
            "    Found existing installation: tqdm 4.66.5\n",
            "    Uninstalling tqdm-4.66.5:\n",
            "      Successfully uninstalled tqdm-4.66.5\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.26.4\n",
            "    Uninstalling numpy-1.26.4:\n",
            "      Successfully uninstalled numpy-1.26.4\n",
            "  Attempting uninstall: einops\n",
            "    Found existing installation: einops 0.8.0\n",
            "    Uninstalling einops-0.8.0:\n",
            "      Successfully uninstalled einops-0.8.0\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.13.1\n",
            "    Uninstalling scipy-1.13.1:\n",
            "      Successfully uninstalled scipy-1.13.1\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 2.1.4\n",
            "    Uninstalling pandas-2.1.4:\n",
            "      Successfully uninstalled pandas-2.1.4\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.3.1+cu121\n",
            "    Uninstalling torch-2.3.1+cu121:\n",
            "      Successfully uninstalled torch-2.3.1+cu121\n",
            "  Attempting uninstall: scikit_learn\n",
            "    Found existing installation: scikit-learn 1.3.2\n",
            "    Uninstalling scikit-learn-1.3.2:\n",
            "      Successfully uninstalled scikit-learn-1.3.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "xgboost 2.1.1 requires nvidia-nccl-cu12; platform_system == \"Linux\" and platform_machine != \"aarch64\", which is not installed.\n",
            "albucore 0.0.13 requires numpy<2,>=1.24.4, but you have numpy 1.23.5 which is incompatible.\n",
            "albumentations 1.4.13 requires numpy>=1.24.4, but you have numpy 1.23.5 which is incompatible.\n",
            "albumentations 1.4.13 requires scipy>=1.10.0, but you have scipy 1.9.1 which is incompatible.\n",
            "chex 0.1.86 requires numpy>=1.24.1, but you have numpy 1.23.5 which is incompatible.\n",
            "google-colab 1.0.0 requires pandas==2.1.4, but you have pandas 2.0.1 which is incompatible.\n",
            "pandas-stubs 2.1.4.231227 requires numpy>=1.26.0; python_version < \"3.13\", but you have numpy 1.23.5 which is incompatible.\n",
            "torchaudio 2.3.1+cu121 requires torch==2.3.1, but you have torch 1.13.1 which is incompatible.\n",
            "torchtext 0.18.0 requires torch>=2.3.0, but you have torch 1.13.1 which is incompatible.\n",
            "torchvision 0.18.1+cu121 requires torch==2.3.1, but you have torch 1.13.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed einops-0.6.1 numpy-1.23.5 nvidia-cublas-cu11-11.10.3.66 nvidia-cuda-nvrtc-cu11-11.7.99 nvidia-cuda-runtime-cu11-11.7.99 nvidia-cudnn-cu11-8.5.0.96 opendatasets-0.1.22 pandas-2.0.1 scikit_learn-1.2.2 scipy-1.9.1 torch-1.13.1 torchscan-0.1.2 tqdm-4.65.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              },
              "id": "6698e54946b043c1a8bb6d763100c5b7"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import random\n",
        "import numpy as np\n",
        "import sys\n",
        "sys.path.append('/content/Transformer-based-solutions-for-the-long-term-time-series-forecasting')\n",
        "\n",
        "from exp.exp_DLinear import Exp_Main as Exp_DLinear\n",
        "\n",
        "from exp.exp_NLinear import Exp_Main as Exp_NLinear\n",
        "\n",
        "from exp.exp_Informer import Exp_Informer\n",
        "\n",
        "from exp.exp_FEDformer import Exp_FEDFormer\n",
        "\n",
        "from exp.exp_PatchTST import Exp_Main as Exp_PatchTST\n"
      ],
      "metadata": {
        "id": "2bH7MqqkKQ_0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if 'Time-Series-Forcasting-Group3' not in sys.path:\n",
        "    sys.path += ['Time-Series-Forcasting-Group3']\n",
        "\n",
        "\n",
        "fix_seed = 2021\n",
        "random.seed(fix_seed)\n",
        "torch.manual_seed(fix_seed)\n",
        "np.random.seed(fix_seed)\n",
        "\n",
        "class dotdict(dict):\n",
        "    \"\"\"dot.notation access to dictionary attributes\"\"\"\n",
        "    __getattr__ = dict.get\n",
        "    __setattr__ = dict.__setitem__\n",
        "    __delattr__ = dict.__delitem__\n",
        "\n",
        "\n",
        "args = dotdict()\n",
        "\n",
        "\n",
        "args.use_multi_gpu = False\n",
        "args.num_workers = 0\n",
        "args.use_gpu = torch.cuda.is_available()\n",
        "\n",
        "\n",
        "args.data = 'custom'\n",
        "args.root_path = './Datasets/CustomData/'\n",
        "args.data_path = 'small.csv'\n",
        "args.features = 'MS'\n",
        "args.target = 'System'\n",
        "args.freq = 'h'\n",
        "args.timeenc = 1\n",
        "args.embed = 'timeF'\n",
        "args.padding = 0"
      ],
      "metadata": {
        "id": "QB0kyNLkKadO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "args.attn = 'prob'\n",
        "args.d_model = 512\n",
        "args.n_heads = 8\n",
        "args.factor = 5\n",
        "args.dropout = 0.1\n",
        "args.d_ff = 2048\n",
        "args.activation = 'gelu'\n",
        "args.mix = True"
      ],
      "metadata": {
        "id": "xJRq5an7KbeV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "args.enc_in = 24\n",
        "args.dec_in = 24\n",
        "args.c_out = 1\n",
        "args.e_layers = 2\n",
        "args.d_layers = 1\n",
        "args.seq_len = 336\n",
        "args.label_len = 48\n",
        "args.pred_len = 96"
      ],
      "metadata": {
        "id": "jPI74hUGKcxT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "args.distil = True\n",
        "args.output_attention = False\n",
        "args.use_amp = False\n",
        "args.train_only = True\n",
        "args.train_epochs = 6\n",
        "args.batch_size = 32\n",
        "args.learning_rate = 0.01\n",
        "args.lradj = 'type1'\n",
        "args.loss = 'mse'\n",
        "args.patience = 3\n",
        "args.des = 'Exp'\n",
        "args.itr = 1"
      ],
      "metadata": {
        "id": "-d8qqS5hKeN2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "args.modes = 32\n",
        "args.moving_avg = [5]\n",
        "args.embed_type = 1\n",
        "\n",
        "# Initialize the model list and experiment classes\n",
        "model_list = {\n",
        "    'DLinear': Exp_DLinear,\n",
        "    'NLinear': Exp_NLinear,\n",
        "    'Informer': Exp_Informer,\n",
        "    'FEDformer': Exp_FEDFormer,\n",
        "    'PatchTST': Exp_PatchTST\n",
        "}"
      ],
      "metadata": {
        "id": "EAgPaqNYKfFP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "print(torch.cuda.is_available())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B0syxKTTKgbD",
        "outputId": "d113ef20-0a99-4a19-fab0-884749de5ef7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "args.gpu = 0  # Specify the first GPU\n"
      ],
      "metadata": {
        "id": "vPL_5ebBKhbf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "import torch\n",
        "from exp.exp_DLinear import Exp_Main\n",
        "\n",
        "# Ensure the project files are accessible\n",
        "project_path = '/content/Transformer-based-solutions-for-the-long-term-time-series-forecasting'\n",
        "if project_path not in sys.path:\n",
        "    sys.path += [project_path]\n",
        "\n",
        "# Define a dotdict to hold hyperparameters\n",
        "class dotdict(dict):\n",
        "    \"\"\"dot.notation access to dictionary attributes\"\"\"\n",
        "    __getattr__ = dict.get\n",
        "    __setattr__ = dict.__setitem__\n",
        "    __delattr__ = dict.__delitem__\n",
        "\n",
        "# Initialize the arguments with your dataset configuration\n",
        "args = dotdict()\n",
        "\n",
        "######################### Device Hyperparameters  ##########################\n",
        "args.use_multi_gpu = False\n",
        "args.num_workers = 0\n",
        "args.use_gpu = torch.cuda.is_available()\n",
        "args.gpu = 0\n",
        "\n",
        "######################### Dataset Hyperparameters  ##########################\n",
        "args.data = 'custom'            # Name your dataset\n",
        "args.root_path = '/content/Transformer-based-solutions-for-the-long-term-time-series-forecasting/Datasets/CustomData/'  # Root path where your dataset is stored\n",
        "args.data_path = 'my.csv'           # Path to your dataset file\n",
        "args.features = 'M'                 # Multivariate predict multivariate\n",
        "args.target = 'System'              # Target feature in your dataset\n",
        "args.freq = 'h'                     # Frequency: hourly data\n",
        "args.embed = 'timeF'                # Time features encoding\n",
        "args.padding = 0                    # Padding (if needed)\n",
        "\n",
        "######################### Experiment Hyperparameters  ##########################\n",
        "args.train_only = True\n",
        "args.train_epochs = 30              # Number of epochs to train\n",
        "args.batch_size = 32                # Batch size for training\n",
        "args.learning_rate = 0.05           # Initial learning rate\n",
        "args.lradj = 'type1'                # Learning rate adjustment strategy\n",
        "args.loss = 'mse'                   # Loss function\n",
        "args.patience = 3                   # Early stopping patience\n",
        "args.des = 'Exp'                    # Experiment description\n",
        "args.itr  = 1\n",
        "\n",
        "# Model-specific hyperparameters\n",
        "args.model = 'DLinear'\n",
        "args.checkpoints = './Checkpoints/DLinear_checkpoints' # Where to save checkpoints\n",
        "args.enc_in = 49                  # Number of input features\n",
        "args.label_len = 48               # Length of the start token series used in the decoder\n",
        "args.seq_len = 336                # Input sequence length\n",
        "args.pred_len = 96                # Prediction length\n",
        "\n",
        "# Set the experiment settings\n",
        "setting = f'{args.model}_train_on_{args.data}_{args.pred_len}_test'\n",
        "print(f\"Hyperparameter Combination of {setting}:\\n\")\n",
        "print(args)\n",
        "\n",
        "# Initialize the experiment\n",
        "Exp = Exp_Main\n",
        "exp = Exp(args)\n",
        "\n",
        "# Train the model\n",
        "exp.train(setting)\n",
        "\n",
        "# Test the model\n",
        "exp.test(setting)\n",
        "\n",
        "# Clear GPU cache\n",
        "torch.cuda.empty_cache()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Ig5Nf5aKip8",
        "outputId": "530759d3-cb75-4088-db59-444f967f1953"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hyperparameter Combination of DLinear_train_on_custom_96_test:\n",
            "\n",
            "{'use_multi_gpu': False, 'num_workers': 0, 'use_gpu': True, 'gpu': 0, 'data': 'custom', 'root_path': '/content/Transformer-based-solutions-for-the-long-term-time-series-forecasting/Datasets/CustomData/', 'data_path': 'my.csv', 'features': 'M', 'target': 'System', 'freq': 'h', 'embed': 'timeF', 'padding': 0, 'train_only': True, 'train_epochs': 30, 'batch_size': 32, 'learning_rate': 0.05, 'lradj': 'type1', 'loss': 'mse', 'patience': 3, 'des': 'Exp', 'itr': 1, 'model': 'DLinear', 'checkpoints': './Checkpoints/DLinear_checkpoints', 'enc_in': 49, 'label_len': 48, 'seq_len': 336, 'pred_len': 96}\n",
            "Use GPU: cuda:0\n",
            "train 68\n",
            "================================================================================\n",
            "                                   Training                               \n",
            "================================================================================\n",
            "                                   Epoch 1 Summery                          \n",
            "--------------------------------------------------------------------------------\n",
            "     Cost time: 1\n",
            "     Steps: 2 \t Train Loss: 32.4198604\n",
            "     Validation loss decreased (inf --> 32.419860).  Saving model ...\n",
            "================================================================================\n",
            "Updating learning rate to 0.05\n",
            "                                   Epoch 2 Summery                          \n",
            "--------------------------------------------------------------------------------\n",
            "     Cost time: 2\n",
            "     Steps: 2 \t Train Loss: 13.1216068\n",
            "     Validation loss decreased (32.419860 --> 13.121607).  Saving model ...\n",
            "================================================================================\n",
            "Updating learning rate to 0.025\n",
            "                                   Epoch 3 Summery                          \n",
            "--------------------------------------------------------------------------------\n",
            "     Cost time: 3\n",
            "     Steps: 2 \t Train Loss: 34.0696659\n",
            "     EarlyStopping counter: 1 out of 3\n",
            "================================================================================\n",
            "Updating learning rate to 0.0125\n",
            "                                   Epoch 4 Summery                          \n",
            "--------------------------------------------------------------------------------\n",
            "     Cost time: 4\n",
            "     Steps: 2 \t Train Loss: 13.0492558\n",
            "     Validation loss decreased (13.121607 --> 13.049256).  Saving model ...\n",
            "================================================================================\n",
            "Updating learning rate to 0.00625\n",
            "                                   Epoch 5 Summery                          \n",
            "--------------------------------------------------------------------------------\n",
            "     Cost time: 5\n",
            "     Steps: 2 \t Train Loss: 4.6014743\n",
            "     Validation loss decreased (13.049256 --> 4.601474).  Saving model ...\n",
            "================================================================================\n",
            "Updating learning rate to 0.003125\n",
            "                                   Epoch 6 Summery                          \n",
            "--------------------------------------------------------------------------------\n",
            "     Cost time: 6\n",
            "     Steps: 2 \t Train Loss: 2.4723607\n",
            "     Validation loss decreased (4.601474 --> 2.472361).  Saving model ...\n",
            "================================================================================\n",
            "Updating learning rate to 0.0015625\n",
            "                                   Epoch 7 Summery                          \n",
            "--------------------------------------------------------------------------------\n",
            "     Cost time: 7\n",
            "     Steps: 2 \t Train Loss: 1.9486514\n",
            "     Validation loss decreased (2.472361 --> 1.948651).  Saving model ...\n",
            "================================================================================\n",
            "Updating learning rate to 0.00078125\n",
            "                                   Epoch 8 Summery                          \n",
            "--------------------------------------------------------------------------------\n",
            "     Cost time: 8\n",
            "     Steps: 2 \t Train Loss: 1.7256069\n",
            "     Validation loss decreased (1.948651 --> 1.725607).  Saving model ...\n",
            "================================================================================\n",
            "Updating learning rate to 0.000390625\n",
            "                                   Epoch 9 Summery                          \n",
            "--------------------------------------------------------------------------------\n",
            "     Cost time: 9\n",
            "     Steps: 2 \t Train Loss: 1.6466970\n",
            "     Validation loss decreased (1.725607 --> 1.646697).  Saving model ...\n",
            "================================================================================\n",
            "Updating learning rate to 0.0001953125\n",
            "                                   Epoch 10 Summery                          \n",
            "--------------------------------------------------------------------------------\n",
            "     Cost time: 10\n",
            "     Steps: 2 \t Train Loss: 1.6059228\n",
            "     Validation loss decreased (1.646697 --> 1.605923).  Saving model ...\n",
            "================================================================================\n",
            "Updating learning rate to 9.765625e-05\n",
            "                                   Epoch 11 Summery                          \n",
            "--------------------------------------------------------------------------------\n",
            "     Cost time: 11\n",
            "     Steps: 2 \t Train Loss: 1.5918850\n",
            "     Validation loss decreased (1.605923 --> 1.591885).  Saving model ...\n",
            "================================================================================\n",
            "Updating learning rate to 4.8828125e-05\n",
            "                                   Epoch 12 Summery                          \n",
            "--------------------------------------------------------------------------------\n",
            "     Cost time: 12\n",
            "     Steps: 2 \t Train Loss: 1.5753279\n",
            "     Validation loss decreased (1.591885 --> 1.575328).  Saving model ...\n",
            "================================================================================\n",
            "Updating learning rate to 2.44140625e-05\n",
            "                                   Epoch 13 Summery                          \n",
            "--------------------------------------------------------------------------------\n",
            "     Cost time: 13\n",
            "     Steps: 2 \t Train Loss: 1.5672814\n",
            "     Validation loss decreased (1.575328 --> 1.567281).  Saving model ...\n",
            "================================================================================\n",
            "Updating learning rate to 1.220703125e-05\n",
            "                                   Epoch 14 Summery                          \n",
            "--------------------------------------------------------------------------------\n",
            "     Cost time: 14\n",
            "     Steps: 2 \t Train Loss: 1.5581147\n",
            "     Validation loss decreased (1.567281 --> 1.558115).  Saving model ...\n",
            "================================================================================\n",
            "Updating learning rate to 6.103515625e-06\n",
            "                                   Epoch 15 Summery                          \n",
            "--------------------------------------------------------------------------------\n",
            "     Cost time: 15\n",
            "     Steps: 2 \t Train Loss: 1.5355626\n",
            "     Validation loss decreased (1.558115 --> 1.535563).  Saving model ...\n",
            "================================================================================\n",
            "Updating learning rate to 3.0517578125e-06\n",
            "                                   Epoch 16 Summery                          \n",
            "--------------------------------------------------------------------------------\n",
            "     Cost time: 16\n",
            "     Steps: 2 \t Train Loss: 1.5306326\n",
            "     Validation loss decreased (1.535563 --> 1.530633).  Saving model ...\n",
            "================================================================================\n",
            "Updating learning rate to 1.52587890625e-06\n",
            "                                   Epoch 17 Summery                          \n",
            "--------------------------------------------------------------------------------\n",
            "     Cost time: 17\n",
            "     Steps: 2 \t Train Loss: 1.5548531\n",
            "     EarlyStopping counter: 1 out of 3\n",
            "================================================================================\n",
            "Updating learning rate to 7.62939453125e-07\n",
            "                                   Epoch 18 Summery                          \n",
            "--------------------------------------------------------------------------------\n",
            "     Cost time: 18\n",
            "     Steps: 2 \t Train Loss: 1.5599712\n",
            "     EarlyStopping counter: 2 out of 3\n",
            "================================================================================\n",
            "Updating learning rate to 3.814697265625e-07\n",
            "                                   Epoch 19 Summery                          \n",
            "--------------------------------------------------------------------------------\n",
            "     Cost time: 19\n",
            "     Steps: 2 \t Train Loss: 1.5559769\n",
            "     EarlyStopping counter: 3 out of 3\n",
            "                      Early stopping                               \n",
            "test 4\n",
            "mae:1.0311957597732544, mse:1.6807745695114136, rmse:1.2964469194412231, mape:5.982047080993652, mspe:2184.741943359375\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "from exp.exp_Informer import Exp_Informer\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "fix_seed = 2021\n",
        "random.seed(fix_seed)\n",
        "torch.manual_seed(fix_seed)\n",
        "np.random.seed(fix_seed)\n",
        "\n",
        "# Define a dotdict to hold hyperparameters\n",
        "class dotdict(dict):\n",
        "    \"\"\"dot.notation access to dictionary attributes\"\"\"\n",
        "    __getattr__ = dict.get\n",
        "    __setattr__ = dict.__setitem__\n",
        "    __delattr__ = dict.__delitem__\n",
        "\n",
        "\"\"\"\n",
        "    **dotdict function**\n",
        "    This function is used to convert a dictionary into\n",
        "    an object whose keys can be accessed as attributes\n",
        "\"\"\"\n",
        "args = dotdict()\n",
        "\n",
        "######################### Custom Dataset Settings #########################\n",
        "args.model = 'informer'  # model of experiment\n",
        "args.train_only = True\n",
        "args.use_multi_gpu = False\n",
        "args.use_gpu = True if torch.cuda.is_available() else False\n",
        "args.learning_rate = 0.0001\n",
        "args.pred_len = 96  # initial prediction sequence length\n",
        "args.label_len = 48  # start token length of Informer decoder\n",
        "args.seq_len = 336  # input sequence length of Informer encoder\n",
        "\n",
        "# Whether to use automatic mixed precision training\n",
        "args.use_amp = False\n"
      ],
      "metadata": {
        "id": "lA8U5KE0LMIQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dlinear"
      ],
      "metadata": {
        "id": "VE1goIn5Kq9G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import os\n",
        "import sys\n",
        "import torch\n",
        "from exp.exp_DLinear import Exp_Main\n",
        "\n",
        "# Ensure the project files are accessible\n",
        "project_path = '/content/Transformer-based-solutions-for-the-long-term-time-series-forecasting'\n",
        "if project_path not in sys.path:\n",
        "    sys.path += [project_path]\n",
        "\n",
        "# Define a dotdict to hold hyperparameters\n",
        "class dotdict(dict):\n",
        "    \"\"\"dot.notation access to dictionary attributes\"\"\"\n",
        "    __getattr__ = dict.get\n",
        "    __setattr__ = dict.__setitem__\n",
        "    __delattr__ = dict.__delitem__\n",
        "\n",
        "# Initialize the arguments with your dataset configuration\n",
        "args = dotdict()\n",
        "\n",
        "######################### Device Hyperparameters  ##########################\n",
        "args.use_multi_gpu = False\n",
        "args.num_workers = 0\n",
        "args.use_gpu = torch.cuda.is_available()\n",
        "args.gpu = 0\n",
        "\n",
        "######################### Dataset Hyperparameters  ##########################\n",
        "args.data = 'custom'            # Name your dataset\n",
        "args.root_path = '/content/Transformer-based-solutions-for-the-long-term-time-series-forecasting/Datasets/CustomData/'  # Root path where your dataset is stored\n",
        "args.data_path = 'my_.csv'           # Path to your dataset file\n",
        "args.features = 'M'                 # Multivariate predict multivariate\n",
        "args.target = 'System'              # Target feature in your dataset\n",
        "args.freq = 'h'                     # Frequency: hourly data\n",
        "args.embed = 'timeF'                # Time features encoding\n",
        "args.padding = 0                    # Padding (if needed)\n",
        "\n",
        "######################### Experiment Hyperparameters  ##########################\n",
        "args.train_only = True\n",
        "args.train_epochs = 30              # Number of epochs to train\n",
        "args.batch_size = 32                # Batch size for training\n",
        "args.learning_rate = 0.05           # Initial learning rate\n",
        "args.lradj = 'type1'                # Learning rate adjustment strategy\n",
        "args.loss = 'mse'                   # Loss function\n",
        "args.patience = 3                   # Early stopping patience\n",
        "args.des = 'Exp'                    # Experiment description\n",
        "args.itr  = 1\n",
        "\n",
        "# Model-specific hyperparameters\n",
        "args.model = 'DLinear'\n",
        "args.checkpoints = './Checkpoints/DLinear_checkpoints' # Where to save checkpoints\n",
        "args.enc_in = 49                  # Number of input features\n",
        "args.label_len = 48               # Length of the start token series used in the decoder\n",
        "args.seq_len = 336                # Input sequence length\n",
        "args.pred_len = 96                # Prediction length\n",
        "\n",
        "# Set the experiment settings\n",
        "setting = f'{args.model}_train_on_{args.data}_{args.pred_len}_test'\n",
        "print(f\"Hyperparameter Combination of {setting}:\\n\")\n",
        "print(args)\n",
        "\n",
        "# Initialize the experiment\n",
        "Exp = Exp_Main\n",
        "exp = Exp(args)\n",
        "\n",
        "# Train the model\n",
        "exp.train(setting)\n",
        "\n",
        "# Test the model\n",
        "exp.test(setting)\n",
        "\n",
        "# Clear GPU cache\n",
        "torch.cuda.empty_cache()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "2Nw0Vn7ELQjV",
        "outputId": "ff4cc636-f757-410c-e67d-5b65858319c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hyperparameter Combination of DLinear_train_on_custom_96_test:\n",
            "\n",
            "{'use_multi_gpu': False, 'num_workers': 0, 'use_gpu': True, 'gpu': 0, 'data': 'custom', 'root_path': '/content/Transformer-based-solutions-for-the-long-term-time-series-forecasting/Datasets/CustomData/', 'data_path': 'my_.csv', 'features': 'M', 'target': 'System', 'freq': 'h', 'embed': 'timeF', 'padding': 0, 'train_only': True, 'train_epochs': 30, 'batch_size': 32, 'learning_rate': 0.05, 'lradj': 'type1', 'loss': 'mse', 'patience': 3, 'des': 'Exp', 'itr': 1, 'model': 'DLinear', 'checkpoints': './Checkpoints/DLinear_checkpoints', 'enc_in': 49, 'label_len': 48, 'seq_len': 336, 'pred_len': 96}\n",
            "Use GPU: cuda:0\n",
            "train 112047\n",
            "================================================================================\n",
            "                                   Training                               \n",
            "================================================================================\n",
            "Epoch: 1, Iters: 500\n",
            "--------------------------------------------------------------------------------\n",
            "    Loss : 0.1911695 (MSE)\n",
            "    Speed: 0.0067 sec/iter \n",
            "    Left time: 702.9522 sec\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch: 1, Iters: 1000\n",
            "--------------------------------------------------------------------------------\n",
            "    Loss : 0.2528623 (MSE)\n",
            "    Speed: 0.0065 sec/iter \n",
            "    Left time: 674.6856 sec\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch: 1, Iters: 1500\n",
            "--------------------------------------------------------------------------------\n",
            "    Loss : 0.5231927 (MSE)\n",
            "    Speed: 0.0065 sec/iter \n",
            "    Left time: 672.4317 sec\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch: 1, Iters: 2000\n",
            "--------------------------------------------------------------------------------\n",
            "    Loss : 0.4630906 (MSE)\n",
            "    Speed: 0.0065 sec/iter \n",
            "    Left time: 672.2202 sec\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch: 1, Iters: 2500\n",
            "--------------------------------------------------------------------------------\n",
            "    Loss : 0.3107968 (MSE)\n",
            "    Speed: 0.0068 sec/iter \n",
            "    Left time: 699.5601 sec\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch: 1, Iters: 3000\n",
            "--------------------------------------------------------------------------------\n",
            "    Loss : 0.3250848 (MSE)\n",
            "    Speed: 0.0061 sec/iter \n",
            "    Left time: 617.8950 sec\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch: 1, Iters: 3500\n",
            "--------------------------------------------------------------------------------\n",
            "    Loss : 0.3435877 (MSE)\n",
            "    Speed: 0.0061 sec/iter \n",
            "    Left time: 616.6923 sec\n",
            "--------------------------------------------------------------------------------\n",
            "                                   Epoch 1 Summery                          \n",
            "--------------------------------------------------------------------------------\n",
            "     Cost time: 1\n",
            "     Steps: 3501 \t Train Loss: 0.6289520\n",
            "     Validation loss decreased (inf --> 0.628952).  Saving model ...\n",
            "================================================================================\n",
            "Updating learning rate to 0.05\n",
            "Epoch: 2, Iters: 500\n",
            "--------------------------------------------------------------------------------\n",
            "    Loss : 0.4554129 (MSE)\n",
            "    Speed: 0.0065 sec/iter \n",
            "    Left time: 660.2448 sec\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch: 2, Iters: 1000\n",
            "--------------------------------------------------------------------------------\n",
            "    Loss : 0.3839698 (MSE)\n",
            "    Speed: 0.0070 sec/iter \n",
            "    Left time: 702.4518 sec\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch: 2, Iters: 1500\n",
            "--------------------------------------------------------------------------------\n",
            "    Loss : 0.5092651 (MSE)\n",
            "    Speed: 0.0066 sec/iter \n",
            "    Left time: 663.8644 sec\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch: 2, Iters: 2000\n",
            "--------------------------------------------------------------------------------\n",
            "    Loss : 0.3853225 (MSE)\n",
            "    Speed: 0.0069 sec/iter \n",
            "    Left time: 688.9055 sec\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch: 2, Iters: 2500\n",
            "--------------------------------------------------------------------------------\n",
            "    Loss : 0.4218066 (MSE)\n",
            "    Speed: 0.0064 sec/iter \n",
            "    Left time: 630.9667 sec\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch: 2, Iters: 3000\n",
            "--------------------------------------------------------------------------------\n",
            "    Loss : 0.2738239 (MSE)\n",
            "    Speed: 0.0065 sec/iter \n",
            "    Left time: 638.8989 sec\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch: 2, Iters: 3500\n",
            "--------------------------------------------------------------------------------\n",
            "    Loss : 0.4508846 (MSE)\n",
            "    Speed: 0.0062 sec/iter \n",
            "    Left time: 611.8202 sec\n",
            "--------------------------------------------------------------------------------\n",
            "                                   Epoch 2 Summery                          \n",
            "--------------------------------------------------------------------------------\n",
            "     Cost time: 2\n",
            "     Steps: 3501 \t Train Loss: 0.4902291\n",
            "     Validation loss decreased (0.628952 --> 0.490229).  Saving model ...\n",
            "================================================================================\n",
            "Updating learning rate to 0.025\n",
            "Epoch: 3, Iters: 500\n",
            "--------------------------------------------------------------------------------\n",
            "    Loss : 0.1918882 (MSE)\n",
            "    Speed: 0.0064 sec/iter \n",
            "    Left time: 624.3286 sec\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch: 3, Iters: 1000\n",
            "--------------------------------------------------------------------------------\n",
            "    Loss : 0.2284267 (MSE)\n",
            "    Speed: 0.0070 sec/iter \n",
            "    Left time: 676.7366 sec\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch: 3, Iters: 1500\n",
            "--------------------------------------------------------------------------------\n",
            "    Loss : 0.3449365 (MSE)\n",
            "    Speed: 0.0064 sec/iter \n",
            "    Left time: 619.6162 sec\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch: 3, Iters: 2000\n",
            "--------------------------------------------------------------------------------\n",
            "    Loss : 0.2646322 (MSE)\n",
            "    Speed: 0.0064 sec/iter \n",
            "    Left time: 614.7248 sec\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch: 3, Iters: 2500\n",
            "--------------------------------------------------------------------------------\n",
            "    Loss : 0.4327415 (MSE)\n",
            "    Speed: 0.0066 sec/iter \n",
            "    Left time: 630.0676 sec\n",
            "--------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-0cba7bf7835b>\u001b[0m in \u001b[0;36m<cell line: 66>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;31m# Train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m \u001b[0mexp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msetting\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;31m# Test the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/Transformer-based-solutions-for-the-long-term-time-series-forecasting/exp/exp_DLinear.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, setting)\u001b[0m\n\u001b[1;32m    122\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m             \u001b[0mepoch_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbatch_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_x_mark\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_y_mark\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    125\u001b[0m                 \u001b[0miter_count\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m                 \u001b[0mmodel_optim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    626\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 628\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    629\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    630\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    669\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    670\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 671\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    672\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    673\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m    263\u001b[0m             \u001b[0;34m>>\u001b[0m\u001b[0;34m>\u001b[0m \u001b[0mdefault_collate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Handle `CustomType` automatically\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m     \"\"\"\n\u001b[0;32m--> 265\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcollate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdefault_collate_fn_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 143\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcollate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msamples\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtransposed\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# Backwards compatibility.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    144\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 143\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcollate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msamples\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtransposed\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# Backwards compatibility.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    144\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcollate_fn_map\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0melem_type\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0melem_type\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcollate_type\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36mcollate_numpy_array_fn\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    170\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdefault_collate_err_msg_format\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcollate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcollate_fn_map\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0melem_type\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0melem_type\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcollate_type\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36mcollate_tensor_fn\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    161\u001b[0m         \u001b[0mstorage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0melem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_new_shared\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0melem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Neu1ExlbKtR9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Clear GPU cache\n",
        "torch.cuda.empty_cache()\n",
        " # Replace 'optimizer' with your variable name\n",
        "\n",
        "# Call garbage collector to free up memory\n",
        "import gc\n",
        "gc.collect()\n",
        "\n",
        "# Clear cache again just to be sure\n",
        "torch.cuda.empty_cache()\n"
      ],
      "metadata": {
        "id": "uEanDmN42lpR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vzwnnwEa2sdj",
        "outputId": "4193c569-2a40-4533-b054-95a71c3e01c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tue Aug 13 17:48:21 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   68C    P0              30W /  70W |    721MiB / 15360MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PatchTST"
      ],
      "metadata": {
        "id": "WmFmT98XKybc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import os\n",
        "from exp.exp_PatchTST import Exp_Main\n",
        "\n",
        "\n",
        "class dotdict(dict):\n",
        "    \"\"\"dot.notation access to dictionary attributes\"\"\"\n",
        "    __getattr__ = dict.get\n",
        "    __setattr__ = dict.__setitem__\n",
        "    __delattr__ = dict.__delitem__\n",
        "\n",
        "\n",
        "fix_seed = 2021\n",
        "torch.manual_seed(fix_seed)\n",
        "np.random.seed(fix_seed)\n",
        "\n",
        "\n",
        "args = dotdict()\n",
        "\n",
        "args.model = 'PatchTST'  # Model Name\n",
        "args.random_seed = fix_seed\n",
        "args.is_training = 1\n",
        "args.model_id = f\"{args.data}_{args.seq_len}_{args.pred_len}\"\n",
        "args.fc_dropout = 0.3\n",
        "args.head_dropout = 0\n",
        "args.patch_len = 16  # The size of each patch\n",
        "args.num_patch = 42  # The number of input patches (42 or 64)\n",
        "args.stride = 8  # The step size used to slide the patch window across the input time series data during patching\n",
        "args.batch_size = 64  # The size of one batch in training\n",
        "args.learning_rate = 0.0001  # Learning rate\n",
        "args.pred_len = 96  # Prediction sequence length (96, 192, 336, 720)\n",
        "args.patience = 3  # The number of epochs to wait before early stopping\n",
        "args.train_epochs = 20  # Number of epochs in training\n",
        "\n",
        "args.use_multi_gpu = False\n",
        "args.use_gpu = torch.cuda.is_available()  # Use GPU if available\n",
        "args.label_len = 48  # Start token length of PatchTST decoder\n",
        "args.use_amp = False  # Whether to use automatic mixed precision training\n",
        "args.output_attention = False  # Whether to output attention in encoder\n",
        "args.features = 'M'  # Forecasting task, options: [M, S, MS]\n",
        "args.train_only = True\n",
        "args.checkpoints = './Checkpoints/PatchTST_checkpoints'  # Location of model checkpoints\n",
        "\n",
        "\n",
        "args.data = 'custom'  # Indicate custom dataset\n",
        "args.root_path = '/content/Transformer-based-solutions-for-the-long-term-time-series-forecasting/Datasets/CustomData/'  # Adjust to your data location\n",
        "args.data_path = 'my_.csv'  # Name of your dataset file\n",
        "args.target = 'System'  # Target feature to predict\n",
        "args.freq = 'h'  # Frequency for time features encoding\n",
        "args.seq_len = 336  # Input sequence length of PatchTST encoder\n",
        "\n",
        "\n",
        "args.enc_in = 49\n",
        "args.dec_in = 49\n",
        "args.c_out = 49\n",
        "args.factor = 5\n",
        "args.d_model = 512\n",
        "args.n_heads = 16\n",
        "args.e_layers = 2\n",
        "args.d_layers = 1  # Number of decoder layers\n",
        "args.d_ff = 2048  # Dimension of feedforward network\n",
        "args.dropout = 0.1  # Dropout rate\n",
        "args.attn = 'prob'  # Attention used in encoder, options: [prob, full]\n",
        "args.embed = 'timeF'  # Time features encoding\n",
        "args.activation = 'gelu'  # Activation function\n",
        "args.distil = True  # Whether to use distilling in encoder\n",
        "args.mix = True  # Apply a linear projection to the concatenated outputs of the attention heads\n",
        "args.padding = 0\n",
        "args.loss = 'mse'  # Evaluating criteria\n",
        "args.lradj = 'type1'  # Learning rate adjustment strategy\n",
        "args.num_workers = 0  # Number of workers for data loading\n",
        "args.itr = 1\n",
        "args.des = \"Exp\"  # Description of the experiment\n",
        "args.gpu = 0  # Default GPU index\n",
        "args.devices = '0,1,2,3'  # Devices for multi-GPU usage\n",
        "\n",
        "\n",
        "args.use_gpu = torch.cuda.is_available() and args.use_gpu\n",
        "if args.use_gpu and args.use_multi_gpu:\n",
        "    args.devices = args.devices.replace(' ', '')\n",
        "    device_ids = args.devices.split(',')\n",
        "    args.device_ids = [int(id_) for id_ in device_ids]\n",
        "    args.gpu = args.device_ids[0]\n",
        "\n",
        "\n",
        "print(\"Hyperparameter Combination of Trail 1:\")\n",
        "print(args)\n",
        "\n",
        "\n",
        "Exp = Exp_Main\n",
        "setting = f'PatchTST_train_on_{args.data}_{args.pred_len}_{args.num_patch}'\n",
        "exp = Exp(args)\n",
        "exp.train(setting)\n",
        "\n",
        "\n",
        "exp.test(setting)\n",
        "\n",
        "\n",
        "torch.cuda.empty_cache()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hTCY5Twt0dHD",
        "outputId": "98c92557-003d-4a3c-d359-8f37722476ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hyperparameter Combination of Trail 1:\n",
            "{'model': 'PatchTST', 'random_seed': 2021, 'is_training': 1, 'model_id': 'None_None_None', 'fc_dropout': 0.3, 'head_dropout': 0, 'patch_len': 16, 'num_patch': 42, 'stride': 8, 'batch_size': 64, 'learning_rate': 0.0001, 'pred_len': 96, 'patience': 3, 'train_epochs': 20, 'use_multi_gpu': False, 'use_gpu': True, 'label_len': 48, 'use_amp': False, 'output_attention': False, 'features': 'M', 'train_only': True, 'checkpoints': './Checkpoints/PatchTST_checkpoints', 'data': 'custom', 'root_path': '/content/Transformer-based-solutions-for-the-long-term-time-series-forecasting/Datasets/CustomData/', 'data_path': 'my_.csv', 'target': 'System', 'freq': 'h', 'seq_len': 336, 'enc_in': 49, 'dec_in': 49, 'c_out': 49, 'factor': 5, 'd_model': 512, 'n_heads': 16, 'e_layers': 2, 'd_layers': 1, 'd_ff': 2048, 'dropout': 0.1, 'attn': 'prob', 'embed': 'timeF', 'activation': 'gelu', 'distil': True, 'mix': True, 'padding': 0, 'loss': 'mse', 'lradj': 'type1', 'num_workers': 0, 'itr': 1, 'des': 'Exp', 'gpu': 0, 'devices': '0,1,2,3'}\n",
            "Use GPU: cuda:0\n",
            "train 78303\n",
            "val 11154\n",
            "test 22400\n",
            "\titers: 100, epoch: 1 | loss: 0.3339831\n",
            "\tspeed: 0.5923s/iter; left time: 14429.0986s\n",
            "\titers: 200, epoch: 1 | loss: 0.3030308\n",
            "\tspeed: 0.5872s/iter; left time: 14246.1979s\n",
            "\titers: 300, epoch: 1 | loss: 0.2727193\n",
            "\tspeed: 0.5872s/iter; left time: 14186.9763s\n",
            "\titers: 400, epoch: 1 | loss: 0.2614269\n",
            "\tspeed: 0.5872s/iter; left time: 14128.0301s\n",
            "\titers: 500, epoch: 1 | loss: 0.2690246\n",
            "\tspeed: 0.5871s/iter; left time: 14068.2584s\n",
            "\titers: 600, epoch: 1 | loss: 0.2803508\n",
            "\tspeed: 0.5871s/iter; left time: 14007.8931s\n",
            "\titers: 700, epoch: 1 | loss: 0.2712775\n",
            "\tspeed: 0.5870s/iter; left time: 13948.1983s\n",
            "\titers: 800, epoch: 1 | loss: 0.2793633\n",
            "\tspeed: 0.5869s/iter; left time: 13886.1806s\n",
            "\titers: 900, epoch: 1 | loss: 0.2546121\n",
            "\tspeed: 0.5870s/iter; left time: 13831.2842s\n",
            "\titers: 1000, epoch: 1 | loss: 0.2519241\n",
            "\tspeed: 0.5874s/iter; left time: 13780.4272s\n",
            "\titers: 1100, epoch: 1 | loss: 0.2288767\n",
            "\tspeed: 0.5871s/iter; left time: 13714.7241s\n",
            "\titers: 1200, epoch: 1 | loss: 0.2413803\n",
            "\tspeed: 0.5871s/iter; left time: 13655.9550s\n",
            "Epoch: 1 cost time: 718.571831703186\n",
            "Epoch: 1, Steps: 1223 | Train Loss: 0.2826850 Vali Loss: 0.3025914 Test Loss: 0.3361978\n",
            ">>> Validation loss decreased (inf --> 0.302591).  Saving model ...\n",
            "Updating learning rate to 0.0001\n",
            "\titers: 100, epoch: 2 | loss: 0.2737644\n",
            "\tspeed: 1.8557s/iter; left time: 42937.8635s\n",
            "\titers: 200, epoch: 2 | loss: 0.2303001\n",
            "\tspeed: 0.5871s/iter; left time: 13525.9435s\n",
            "\titers: 300, epoch: 2 | loss: 0.2268902\n",
            "\tspeed: 0.5871s/iter; left time: 13467.4700s\n",
            "\titers: 400, epoch: 2 | loss: 0.2453172\n",
            "\tspeed: 0.5872s/iter; left time: 13409.8405s\n",
            "\titers: 500, epoch: 2 | loss: 0.2578215\n",
            "\tspeed: 0.5870s/iter; left time: 13348.0989s\n",
            "\titers: 600, epoch: 2 | loss: 0.2350404\n",
            "\tspeed: 0.5870s/iter; left time: 13288.5034s\n",
            "\titers: 700, epoch: 2 | loss: 0.2124314\n",
            "\tspeed: 0.5877s/iter; left time: 13245.1672s\n",
            "\titers: 800, epoch: 2 | loss: 0.2208656\n",
            "\tspeed: 0.5874s/iter; left time: 13178.9822s\n",
            "\titers: 900, epoch: 2 | loss: 0.2218328\n",
            "\tspeed: 0.5871s/iter; left time: 13114.2495s\n",
            "\titers: 1000, epoch: 2 | loss: 0.2255224\n",
            "\tspeed: 0.5870s/iter; left time: 13054.1079s\n",
            "\titers: 1100, epoch: 2 | loss: 0.2029869\n",
            "\tspeed: 0.5870s/iter; left time: 12995.8232s\n",
            "\titers: 1200, epoch: 2 | loss: 0.2213060\n",
            "\tspeed: 0.5870s/iter; left time: 12936.1363s\n",
            "Epoch: 2 cost time: 717.7149436473846\n",
            "Epoch: 2, Steps: 1223 | Train Loss: 0.2308777 Vali Loss: 0.5228370 Test Loss: 0.6239389\n",
            "EarlyStopping counter: 1 out of 3\n",
            "Updating learning rate to 5e-05\n",
            "\titers: 100, epoch: 3 | loss: 0.2374241\n",
            "\tspeed: 1.8520s/iter; left time: 40587.3639s\n",
            "\titers: 200, epoch: 3 | loss: 0.2268295\n",
            "\tspeed: 0.5871s/iter; left time: 12808.1515s\n",
            "\titers: 300, epoch: 3 | loss: 0.2103527\n",
            "\tspeed: 0.5871s/iter; left time: 12747.8922s\n",
            "\titers: 400, epoch: 3 | loss: 0.2270112\n",
            "\tspeed: 0.5872s/iter; left time: 12692.9731s\n",
            "\titers: 500, epoch: 3 | loss: 0.2322087\n",
            "\tspeed: 0.5870s/iter; left time: 12629.5825s\n",
            "\titers: 600, epoch: 3 | loss: 0.2119065\n",
            "\tspeed: 0.5870s/iter; left time: 12570.8771s\n",
            "\titers: 700, epoch: 3 | loss: 0.1903004\n",
            "\tspeed: 0.5869s/iter; left time: 12509.8341s\n",
            "\titers: 800, epoch: 3 | loss: 0.2020590\n",
            "\tspeed: 0.5870s/iter; left time: 12453.7683s\n",
            "\titers: 900, epoch: 3 | loss: 0.2210016\n",
            "\tspeed: 0.5871s/iter; left time: 12396.7627s\n",
            "\titers: 1000, epoch: 3 | loss: 0.2151037\n",
            "\tspeed: 0.5870s/iter; left time: 12336.2473s\n",
            "\titers: 1100, epoch: 3 | loss: 0.2025849\n",
            "\tspeed: 0.5871s/iter; left time: 12278.1688s\n",
            "\titers: 1200, epoch: 3 | loss: 0.1986762\n",
            "\tspeed: 0.5870s/iter; left time: 12217.9768s\n",
            "Epoch: 3 cost time: 717.6189525127411\n",
            "Epoch: 3, Steps: 1223 | Train Loss: 0.2137905 Vali Loss: 0.3076554 Test Loss: 0.3395751\n",
            "EarlyStopping counter: 2 out of 3\n",
            "Updating learning rate to 2.5e-05\n",
            "\titers: 100, epoch: 4 | loss: 0.1990977\n",
            "\tspeed: 1.8520s/iter; left time: 38322.3659s\n",
            "\titers: 200, epoch: 4 | loss: 0.1994249\n",
            "\tspeed: 0.5871s/iter; left time: 12089.6200s\n",
            "\titers: 300, epoch: 4 | loss: 0.2091633\n",
            "\tspeed: 0.5870s/iter; left time: 12029.1074s\n",
            "\titers: 400, epoch: 4 | loss: 0.2010631\n",
            "\tspeed: 0.5870s/iter; left time: 11971.0174s\n",
            "\titers: 500, epoch: 4 | loss: 0.2063875\n",
            "\tspeed: 0.5869s/iter; left time: 11909.1363s\n",
            "\titers: 600, epoch: 4 | loss: 0.2164150\n",
            "\tspeed: 0.5870s/iter; left time: 11852.9957s\n",
            "\titers: 700, epoch: 4 | loss: 0.2127997\n",
            "\tspeed: 0.5870s/iter; left time: 11794.9562s\n",
            "\titers: 800, epoch: 4 | loss: 0.2095258\n",
            "\tspeed: 0.5871s/iter; left time: 11737.8537s\n",
            "\titers: 900, epoch: 4 | loss: 0.2097777\n",
            "\tspeed: 0.5871s/iter; left time: 11678.0100s\n",
            "\titers: 1000, epoch: 4 | loss: 0.2129274\n",
            "\tspeed: 0.5870s/iter; left time: 11618.4471s\n",
            "\titers: 1100, epoch: 4 | loss: 0.2027845\n",
            "\tspeed: 0.5869s/iter; left time: 11557.3996s\n",
            "\titers: 1200, epoch: 4 | loss: 0.2360739\n",
            "\tspeed: 0.5869s/iter; left time: 11498.1151s\n",
            "Epoch: 4 cost time: 717.5787825584412\n",
            "Epoch: 4, Steps: 1223 | Train Loss: 0.2075267 Vali Loss: 0.3920856 Test Loss: 0.4551173\n",
            "EarlyStopping counter: 3 out of 3\n",
            "Early stopping\n",
            "test 22400\n",
            "mae:0.4020240306854248, mse:0.3361984193325043, rmse:0.5798261761665344, mape:1.7010385990142822, mspe:13098.736328125\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# FedFormer"
      ],
      "metadata": {
        "id": "BTJqTdNIbc4p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!pip install einops\n",
        "from exp.exp_FEDformer import Exp_FEDFormer\n",
        "# Set random seeds for reproducibility\n",
        "fix_seed = 2021\n",
        "random.seed(fix_seed)\n",
        "torch.manual_seed(fix_seed)\n",
        "np.random.seed(fix_seed)\n",
        "\n",
        "# Define a dotdict to hold hyperparameters\n",
        "class dotdict(dict):\n",
        "    \"\"\"dot.notation access to dictionary attributes\"\"\"\n",
        "    __getattr__ = dict.get\n",
        "    __setattr__ = dict.__setitem__\n",
        "    __delattr__ = dict.__delitem__\n",
        "\n",
        "\"\"\"\n",
        "    **dotdict function**\n",
        "    This function is used to convert a dictionary into\n",
        "    an object whose keys can be accessed as attributes\n",
        "\"\"\"\n",
        "args = dotdict()\n",
        "\n",
        "\n",
        "args.model = 'FEDformer'  # model of experiment\n",
        "args.train_only = True\n",
        "args.use_multi_gpu = False\n",
        "args.use_gpu = True if torch.cuda.is_available() else False\n",
        "args.learning_rate = 0.005\n",
        "args.pred_len = 96  # prediction sequence length\n",
        "args.label_len = 48  # start token length of FEDformer decoder\n",
        "args.seq_len = 96  # input sequence length of FEDformer encoder\n",
        "\n",
        "\n",
        "args.use_amp = False\n",
        "args.output_attention = False  # whether to output attention in encoder\n",
        "args.features = 'M'  # forecasting task, options: [M, S, MS]; M: multivariate predict multivariate\n",
        "args.train_only = True\n",
        "args.checkpoints = './Checkpoints/FEDFormer_checkpoints'  # location of model checkpoints\n",
        "args.patience = 3\n",
        "args.train_epochs = 6\n",
        "\n",
        "\n",
        "args.data = 'custom'  # Name your dataset\n",
        "args.root_path = '/content/Transformer-based-solutions-for-the-long-term-time-series-forecasting/Datasets/CustomData/'  # Root path where your dataset is stored\n",
        "args.data_path = 'my_.csv'  # Path to your dataset file\n",
        "args.target = 'System'  # Target feature in your dataset\n",
        "args.freq = 'h'  # Frequency for time features encoding\n",
        "\n",
        "\n",
        "args.enc_in = 49  # encoder input size (number of features in your dataset)\n",
        "args.dec_in = 49  # decoder input size (number of features in your dataset)\n",
        "args.c_out = 49  # output size (number of features in your dataset)\n",
        "args.factor = 5  # probsparse attn factor\n",
        "args.d_model = 512  # dimension of model\n",
        "args.n_heads = 8  # number of heads\n",
        "args.e_layers = 2  # number of encoder layers\n",
        "args.d_layers = 1  # number of decoder layers\n",
        "args.d_ff = 2048  # dimension of FCN in model\n",
        "args.dropout = 0.05  # dropout\n",
        "args.attn = 'prob'  # attention used in encoder, options: [prob, full]\n",
        "args.embed = 'timeF'  # time features encoding, options: [timeF, fixed, learned]\n",
        "args.activation = 'gelu'  # activation function\n",
        "args.distil = True  # whether to use distilling in encoder\n",
        "args.mix = True\n",
        "args.padding = 0\n",
        "args.batch_size = 32\n",
        "args.loss = 'mse'\n",
        "args.lradj = 'type1'\n",
        "args.num_workers = 0\n",
        "args.des = 'exp'\n",
        "args.gpu = 0\n",
        "args.modes = 32\n",
        "args.moving_avg = [12]\n",
        "args.embed_type = 1\n",
        "\n",
        "\n",
        "args.use_gpu = True if torch.cuda.is_available() and args.use_gpu else False\n",
        "if args.use_gpu and args.use_multi_gpu:\n",
        "    args.devices = args.devices.replace(' ', '')\n",
        "    device_ids = args.devices.split(',')\n",
        "    args.device_ids = [int(id_) for id_ in device_ids]\n",
        "    args.gpu = args.device_ids[0]\n",
        "\n",
        "\n",
        "setting = f'{args.model}_train_on_{args.data}_{args.pred_len}_test'\n",
        "print(f\"Hyperparameter Combination of {setting}:\\n\")\n",
        "print(args)\n",
        "\n",
        "\n",
        "Exp = Exp_FEDFormer\n",
        "exp = Exp(args)\n",
        "\n",
        "\n",
        "exp.train(setting)\n",
        "\n",
        "\n",
        "exp.test(setting)\n",
        "\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "args.pred_len = 192\n",
        "setting = f'{args.model}_train_on_{args.data}_{args.pred_len}'\n",
        "print(f\"Hyperparameter Combination of {setting}:\\n\")\n",
        "print(args)\n",
        "\n",
        "exp = Exp(args)\n",
        "\n",
        "exp.train(setting)"
      ],
      "metadata": {
        "id": "fLMWCoa42EsI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "72f22c1f-027a-4c4b-b02e-85132d83c981"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: einops in /usr/local/lib/python3.10/dist-packages (0.6.1)\n",
            "Hyperparameter Combination of FEDformer_train_on_custom_96_test:\n",
            "\n",
            "{'model': 'FEDformer', 'train_only': True, 'use_multi_gpu': False, 'use_gpu': True, 'learning_rate': 0.005, 'pred_len': 96, 'label_len': 48, 'seq_len': 96, 'use_amp': False, 'output_attention': False, 'features': 'M', 'checkpoints': './Checkpoints/FEDFormer_checkpoints', 'patience': 3, 'train_epochs': 6, 'data': 'custom', 'root_path': '/content/Transformer-based-solutions-for-the-long-term-time-series-forecasting/Datasets/CustomData/', 'data_path': 'my_.csv', 'target': 'System', 'freq': 'h', 'enc_in': 49, 'dec_in': 49, 'c_out': 49, 'factor': 5, 'd_model': 512, 'n_heads': 8, 'e_layers': 2, 'd_layers': 1, 'd_ff': 2048, 'dropout': 0.05, 'attn': 'prob', 'embed': 'timeF', 'activation': 'gelu', 'distil': True, 'mix': True, 'padding': 0, 'batch_size': 32, 'loss': 'mse', 'lradj': 'type1', 'num_workers': 0, 'des': 'exp', 'gpu': 0, 'modes': 32, 'moving_avg': [12], 'embed_type': 1}\n",
            "Use GPU: cuda:0\n",
            "train 78543\n",
            "val 11154\n",
            "test 22400\n",
            "\titers:  100, epoch: 1 | loss: 15.8516273\n",
            "\tspeed: 0.1106sec/iter | left time: 1617.0588sec\n",
            "\titers:  200, epoch: 1 | loss: 3.9732599\n",
            "\tspeed: 0.1077sec/iter | left time: 1564.6557sec\n",
            "\titers:  300, epoch: 1 | loss: 2.3348985\n",
            "\tspeed: 0.1077sec/iter | left time: 1554.2385sec\n",
            "\titers:  400, epoch: 1 | loss: 1.8219308\n",
            "\tspeed: 0.1075sec/iter | left time: 1540.2315sec\n",
            "\titers:  500, epoch: 1 | loss: 1.7989231\n",
            "\tspeed: 0.1068sec/iter | left time: 1518.8664sec\n",
            "\titers:  600, epoch: 1 | loss: 1.3417794\n",
            "\tspeed: 0.1085sec/iter | left time: 1532.6652sec\n",
            "\titers:  700, epoch: 1 | loss: 1.0624877\n",
            "\tspeed: 0.1084sec/iter | left time: 1520.7714sec\n",
            "\titers:  800, epoch: 1 | loss: 0.9754355\n",
            "\tspeed: 0.1053sec/iter | left time: 1466.6171sec\n",
            "\titers:  900, epoch: 1 | loss: 0.8437341\n",
            "\tspeed: 0.1022sec/iter | left time: 1412.6563sec\n",
            "\titers: 1000, epoch: 1 | loss: 0.7969875\n",
            "\tspeed: 0.1028sec/iter | left time: 1410.9520sec\n",
            "\titers: 1100, epoch: 1 | loss: 0.7358975\n",
            "\tspeed: 0.1033sec/iter | left time: 1407.4258sec\n",
            "\titers: 1200, epoch: 1 | loss: 0.6651489\n",
            "\tspeed: 0.1029sec/iter | left time: 1391.2851sec\n",
            "\titers: 1300, epoch: 1 | loss: 0.6061899\n",
            "\tspeed: 0.1033sec/iter | left time: 1386.8037sec\n",
            "\titers: 1400, epoch: 1 | loss: 0.5367639\n",
            "\tspeed: 0.1059sec/iter | left time: 1410.9068sec\n",
            "\titers: 1500, epoch: 1 | loss: 0.5792010\n",
            "\tspeed: 0.1080sec/iter | left time: 1428.4579sec\n",
            "\titers: 1600, epoch: 1 | loss: 0.5247957\n",
            "\tspeed: 0.1070sec/iter | left time: 1404.2954sec\n",
            "\titers: 1700, epoch: 1 | loss: 0.4765894\n",
            "\tspeed: 0.1075sec/iter | left time: 1400.4745sec\n",
            "\titers: 1800, epoch: 1 | loss: 0.4842429\n",
            "\tspeed: 0.1075sec/iter | left time: 1390.0444sec\n",
            "\titers: 1900, epoch: 1 | loss: 0.3942532\n",
            "\tspeed: 0.1078sec/iter | left time: 1383.1415sec\n",
            "\titers: 2000, epoch: 1 | loss: 0.4071272\n",
            "\tspeed: 0.1070sec/iter | left time: 1361.6062sec\n",
            "\titers: 2100, epoch: 1 | loss: 0.4356134\n",
            "\tspeed: 0.1068sec/iter | left time: 1348.1488sec\n",
            "\titers: 2200, epoch: 1 | loss: 0.4010864\n",
            "\tspeed: 0.1069sec/iter | left time: 1339.1406sec\n",
            "\titers: 2300, epoch: 1 | loss: 0.3773723\n",
            "\tspeed: 0.1066sec/iter | left time: 1324.6386sec\n",
            "\titers: 2400, epoch: 1 | loss: 0.3835875\n",
            "\tspeed: 0.1071sec/iter | left time: 1319.5529sec\n",
            "Epoch: 1 cost time: 261.43156337738037\n",
            "Epoch: 1, Steps: 2454 | Train Loss: 124.2733478 Vali Loss: 0.3663321 Test Loss: 0.4688961\n",
            ">>> Validation loss decreased (inf --> 0.366332).  Saving model ...\n",
            "Updating learning rate to 0.005\n",
            "\titers:  100, epoch: 2 | loss: 0.4045595\n",
            "\tspeed: 0.4493sec/iter | left time: 5468.2824sec\n",
            "\titers:  200, epoch: 2 | loss: 0.3673031\n",
            "\tspeed: 0.1067sec/iter | left time: 1287.7448sec\n",
            "\titers:  300, epoch: 2 | loss: 0.3515512\n",
            "\tspeed: 0.1067sec/iter | left time: 1277.6115sec\n",
            "\titers:  400, epoch: 2 | loss: 0.3964467\n",
            "\tspeed: 0.1070sec/iter | left time: 1269.8080sec\n",
            "\titers:  500, epoch: 2 | loss: 0.3390115\n",
            "\tspeed: 0.1073sec/iter | left time: 1263.2393sec\n",
            "\titers:  600, epoch: 2 | loss: 0.3383428\n",
            "\tspeed: 0.1068sec/iter | left time: 1246.9211sec\n",
            "\titers:  700, epoch: 2 | loss: 0.3286211\n",
            "\tspeed: 0.1064sec/iter | left time: 1231.6897sec\n",
            "\titers:  800, epoch: 2 | loss: 0.3448320\n",
            "\tspeed: 0.1064sec/iter | left time: 1220.9024sec\n",
            "\titers:  900, epoch: 2 | loss: 0.3531426\n",
            "\tspeed: 0.1071sec/iter | left time: 1217.5599sec\n",
            "\titers: 1000, epoch: 2 | loss: 0.3290027\n",
            "\tspeed: 0.1067sec/iter | left time: 1202.0601sec\n",
            "\titers: 1100, epoch: 2 | loss: 0.3681610\n",
            "\tspeed: 0.1066sec/iter | left time: 1190.7222sec\n",
            "\titers: 1200, epoch: 2 | loss: 0.3269644\n",
            "\tspeed: 0.1068sec/iter | left time: 1182.7141sec\n",
            "\titers: 1300, epoch: 2 | loss: 0.3177025\n",
            "\tspeed: 0.1061sec/iter | left time: 1163.9157sec\n",
            "\titers: 1400, epoch: 2 | loss: 0.3112862\n",
            "\tspeed: 0.1022sec/iter | left time: 1111.4889sec\n",
            "\titers: 1500, epoch: 2 | loss: 0.3311329\n",
            "\tspeed: 0.1039sec/iter | left time: 1119.4972sec\n",
            "\titers: 1600, epoch: 2 | loss: 0.2951836\n",
            "\tspeed: 0.1024sec/iter | left time: 1092.3086sec\n",
            "\titers: 1700, epoch: 2 | loss: 0.2647825\n",
            "\tspeed: 0.1013sec/iter | left time: 1070.6337sec\n",
            "\titers: 1800, epoch: 2 | loss: 0.2969720\n",
            "\tspeed: 0.1026sec/iter | left time: 1074.1530sec\n",
            "\titers: 1900, epoch: 2 | loss: 0.2805078\n",
            "\tspeed: 0.1046sec/iter | left time: 1084.2930sec\n",
            "\titers: 2000, epoch: 2 | loss: 0.2718980\n",
            "\tspeed: 0.1045sec/iter | left time: 1073.1828sec\n",
            "\titers: 2100, epoch: 2 | loss: 0.2470703\n",
            "\tspeed: 0.1067sec/iter | left time: 1085.4318sec\n",
            "\titers: 2200, epoch: 2 | loss: 0.3044215\n",
            "\tspeed: 0.1062sec/iter | left time: 1069.1529sec\n",
            "\titers: 2300, epoch: 2 | loss: 0.2469307\n",
            "\tspeed: 0.1077sec/iter | left time: 1074.1362sec\n",
            "\titers: 2400, epoch: 2 | loss: 0.2961678\n",
            "\tspeed: 0.1083sec/iter | left time: 1069.2470sec\n",
            "Epoch: 2 cost time: 259.6345477104187\n",
            "Epoch: 2, Steps: 2454 | Train Loss: 0.3191338 Vali Loss: 0.3431772 Test Loss: 0.4436384\n",
            ">>> Validation loss decreased (0.366332 --> 0.343177).  Saving model ...\n",
            "Updating learning rate to 0.0025\n",
            "\titers:  100, epoch: 3 | loss: 0.2941737\n",
            "\tspeed: 0.4562sec/iter | left time: 4432.5086sec\n",
            "\titers:  200, epoch: 3 | loss: 0.3000975\n",
            "\tspeed: 0.1082sec/iter | left time: 1040.8972sec\n",
            "\titers:  300, epoch: 3 | loss: 0.2876708\n",
            "\tspeed: 0.1080sec/iter | left time: 1028.1946sec\n",
            "\titers:  400, epoch: 3 | loss: 0.2669379\n",
            "\tspeed: 0.1083sec/iter | left time: 1019.6646sec\n",
            "\titers:  500, epoch: 3 | loss: 0.3109673\n",
            "\tspeed: 0.1092sec/iter | left time: 1017.4571sec\n",
            "\titers:  600, epoch: 3 | loss: 0.2635379\n",
            "\tspeed: 0.1086sec/iter | left time: 1001.3305sec\n",
            "\titers:  700, epoch: 3 | loss: 0.2648778\n",
            "\tspeed: 0.1077sec/iter | left time: 982.2425sec\n",
            "\titers:  800, epoch: 3 | loss: 0.2721594\n",
            "\tspeed: 0.1084sec/iter | left time: 977.4186sec\n",
            "\titers:  900, epoch: 3 | loss: 0.2749460\n",
            "\tspeed: 0.1086sec/iter | left time: 968.3594sec\n",
            "\titers: 1000, epoch: 3 | loss: 0.2482203\n",
            "\tspeed: 0.1093sec/iter | left time: 963.3902sec\n",
            "\titers: 1100, epoch: 3 | loss: 0.2563542\n",
            "\tspeed: 0.1072sec/iter | left time: 934.8715sec\n",
            "\titers: 1200, epoch: 3 | loss: 0.2773424\n",
            "\tspeed: 0.1079sec/iter | left time: 929.4234sec\n",
            "\titers: 1300, epoch: 3 | loss: 0.2961571\n",
            "\tspeed: 0.1093sec/iter | left time: 930.7819sec\n",
            "\titers: 1400, epoch: 3 | loss: 0.2727240\n",
            "\tspeed: 0.1088sec/iter | left time: 915.4373sec\n",
            "\titers: 1500, epoch: 3 | loss: 0.2600051\n",
            "\tspeed: 0.1088sec/iter | left time: 905.2563sec\n",
            "\titers: 1600, epoch: 3 | loss: 0.2610602\n",
            "\tspeed: 0.1070sec/iter | left time: 878.9580sec\n",
            "\titers: 1700, epoch: 3 | loss: 0.2672831\n",
            "\tspeed: 0.1079sec/iter | left time: 875.7900sec\n",
            "\titers: 1800, epoch: 3 | loss: 3970034.0000000\n",
            "\tspeed: 0.1078sec/iter | left time: 863.8615sec\n",
            "\titers: 1900, epoch: 3 | loss: 52410.8984375\n",
            "\tspeed: 0.1085sec/iter | left time: 859.1131sec\n",
            "\titers: 2000, epoch: 3 | loss: 39829.9687500\n",
            "\tspeed: 0.1064sec/iter | left time: 832.0800sec\n",
            "\titers: 2100, epoch: 3 | loss: 32660.5566406\n",
            "\tspeed: 0.1092sec/iter | left time: 843.0134sec\n",
            "\titers: 2200, epoch: 3 | loss: 26460.9648438\n",
            "\tspeed: 0.1073sec/iter | left time: 817.5005sec\n",
            "\titers: 2300, epoch: 3 | loss: 18913.2792969\n",
            "\tspeed: 0.1072sec/iter | left time: 806.0603sec\n",
            "\titers: 2400, epoch: 3 | loss: 18245.5078125\n",
            "\tspeed: 0.1065sec/iter | left time: 790.2062sec\n",
            "Epoch: 3 cost time: 265.31407403945923\n",
            "Epoch: 3, Steps: 2454 | Train Loss: 2594723.5739404 Vali Loss: 2192.3615723 Test Loss: 2532.6708984\n",
            "EarlyStopping counter: 1 out of 3\n",
            "Updating learning rate to 0.00125\n",
            "\titers:  100, epoch: 4 | loss: 17580.2089844\n",
            "\tspeed: 0.4482sec/iter | left time: 3254.9798sec\n",
            "\titers:  200, epoch: 4 | loss: 15824.5878906\n",
            "\tspeed: 0.1038sec/iter | left time: 743.4946sec\n",
            "\titers:  300, epoch: 4 | loss: 15547.3564453\n",
            "\tspeed: 0.1032sec/iter | left time: 729.1052sec\n",
            "\titers:  400, epoch: 4 | loss: 11569.4882812\n",
            "\tspeed: 0.1046sec/iter | left time: 728.4916sec\n",
            "\titers:  500, epoch: 4 | loss: 11146.2363281\n",
            "\tspeed: 0.1045sec/iter | left time: 717.3457sec\n",
            "\titers:  600, epoch: 4 | loss: 9358.3261719\n",
            "\tspeed: 0.1061sec/iter | left time: 717.6493sec\n",
            "\titers:  700, epoch: 4 | loss: 10914.0048828\n",
            "\tspeed: 0.1065sec/iter | left time: 709.3570sec\n",
            "\titers:  800, epoch: 4 | loss: 9955.7753906\n",
            "\tspeed: 0.1096sec/iter | left time: 719.4192sec\n",
            "\titers:  900, epoch: 4 | loss: 7717.0595703\n",
            "\tspeed: 0.1087sec/iter | left time: 702.4833sec\n",
            "\titers: 1000, epoch: 4 | loss: 8481.5654297\n",
            "\tspeed: 0.1067sec/iter | left time: 679.1700sec\n",
            "\titers: 1100, epoch: 4 | loss: 8325.0898438\n",
            "\tspeed: 0.1029sec/iter | left time: 644.2678sec\n",
            "\titers: 1200, epoch: 4 | loss: 7613.0219727\n",
            "\tspeed: 0.1075sec/iter | left time: 662.7329sec\n",
            "\titers: 1300, epoch: 4 | loss: 6690.1147461\n",
            "\tspeed: 0.1088sec/iter | left time: 659.7781sec\n",
            "\titers: 1400, epoch: 4 | loss: 7307.1054688\n",
            "\tspeed: 0.1086sec/iter | left time: 647.3331sec\n",
            "\titers: 1500, epoch: 4 | loss: 6277.0874023\n",
            "\tspeed: 0.1082sec/iter | left time: 634.3598sec\n",
            "\titers: 1600, epoch: 4 | loss: 5568.6884766\n",
            "\tspeed: 0.1082sec/iter | left time: 623.5243sec\n",
            "\titers: 1700, epoch: 4 | loss: 5824.0742188\n",
            "\tspeed: 0.1072sec/iter | left time: 607.3528sec\n",
            "\titers: 1800, epoch: 4 | loss: 5259.0151367\n",
            "\tspeed: 0.1071sec/iter | left time: 595.5516sec\n",
            "\titers: 1900, epoch: 4 | loss: 5595.8164062\n",
            "\tspeed: 0.1093sec/iter | left time: 597.3510sec\n",
            "\titers: 2000, epoch: 4 | loss: 6679.9399414\n",
            "\tspeed: 0.1080sec/iter | left time: 579.4071sec\n",
            "\titers: 2100, epoch: 4 | loss: 4300.9516602\n",
            "\tspeed: 0.1092sec/iter | left time: 574.7659sec\n",
            "\titers: 2200, epoch: 4 | loss: 4860.4326172\n",
            "\tspeed: 0.1088sec/iter | left time: 561.6941sec\n",
            "\titers: 2300, epoch: 4 | loss: 4001.9416504\n",
            "\tspeed: 0.1080sec/iter | left time: 546.6474sec\n",
            "\titers: 2400, epoch: 4 | loss: 4591.4218750\n",
            "\tspeed: 0.1085sec/iter | left time: 538.3293sec\n",
            "Epoch: 4 cost time: 262.47376894950867\n",
            "Epoch: 4, Steps: 2454 | Train Loss: 8836.5332270 Vali Loss: 492.1288757 Test Loss: 585.9178467\n",
            "EarlyStopping counter: 2 out of 3\n",
            "Updating learning rate to 0.000625\n",
            "\titers:  100, epoch: 5 | loss: 3636.6147461\n",
            "\tspeed: 0.4515sec/iter | left time: 2171.4716sec\n",
            "\titers:  200, epoch: 5 | loss: 3676.3740234\n",
            "\tspeed: 0.1075sec/iter | left time: 506.3403sec\n",
            "\titers:  300, epoch: 5 | loss: 3000.4602051\n",
            "\tspeed: 0.1081sec/iter | left time: 498.4513sec\n",
            "\titers:  400, epoch: 5 | loss: 2963.0903320\n",
            "\tspeed: 0.1084sec/iter | left time: 488.8280sec\n",
            "\titers:  500, epoch: 5 | loss: 3309.9633789\n",
            "\tspeed: 0.1075sec/iter | left time: 473.7527sec\n",
            "\titers:  600, epoch: 5 | loss: 3210.9116211\n",
            "\tspeed: 0.1080sec/iter | left time: 465.4849sec\n",
            "\titers:  700, epoch: 5 | loss: 2864.0620117\n",
            "\tspeed: 0.1087sec/iter | left time: 457.3094sec\n",
            "\titers:  800, epoch: 5 | loss: 2804.1066895\n",
            "\tspeed: 0.1081sec/iter | left time: 444.3531sec\n",
            "\titers:  900, epoch: 5 | loss: 2614.7717285\n",
            "\tspeed: 0.1081sec/iter | left time: 433.3413sec\n",
            "\titers: 1000, epoch: 5 | loss: 3080.2644043\n",
            "\tspeed: 0.1068sec/iter | left time: 417.6414sec\n",
            "\titers: 1100, epoch: 5 | loss: 2668.4245605\n",
            "\tspeed: 0.1078sec/iter | left time: 410.6492sec\n",
            "\titers: 1200, epoch: 5 | loss: 2449.4423828\n",
            "\tspeed: 0.1084sec/iter | left time: 401.9366sec\n",
            "\titers: 1300, epoch: 5 | loss: 2292.8486328\n",
            "\tspeed: 0.1081sec/iter | left time: 389.9985sec\n",
            "\titers: 1400, epoch: 5 | loss: 2094.8498535\n",
            "\tspeed: 0.1041sec/iter | left time: 365.2180sec\n",
            "\titers: 1500, epoch: 5 | loss: 2237.2243652\n",
            "\tspeed: 0.1030sec/iter | left time: 351.0069sec\n",
            "\titers: 1600, epoch: 5 | loss: 2116.2822266\n",
            "\tspeed: 0.1028sec/iter | left time: 340.1445sec\n",
            "\titers: 1700, epoch: 5 | loss: 2211.0661621\n",
            "\tspeed: 0.1028sec/iter | left time: 329.9670sec\n",
            "\titers: 1800, epoch: 5 | loss: 1918.0275879\n",
            "\tspeed: 0.1032sec/iter | left time: 320.7148sec\n",
            "\titers: 1900, epoch: 5 | loss: 1848.4791260\n",
            "\tspeed: 0.1046sec/iter | left time: 314.6138sec\n",
            "\titers: 2000, epoch: 5 | loss: 1520.8875732\n",
            "\tspeed: 0.1041sec/iter | left time: 302.7532sec\n",
            "\titers: 2100, epoch: 5 | loss: 1727.9713135\n",
            "\tspeed: 0.1054sec/iter | left time: 296.1995sec\n",
            "\titers: 2200, epoch: 5 | loss: 1497.9763184\n",
            "\tspeed: 0.1066sec/iter | left time: 288.8639sec\n",
            "\titers: 2300, epoch: 5 | loss: 1493.5225830\n",
            "\tspeed: 0.1090sec/iter | left time: 284.2745sec\n",
            "\titers: 2400, epoch: 5 | loss: 2044.5510254\n",
            "\tspeed: 0.1072sec/iter | left time: 268.9272sec\n",
            "Epoch: 5 cost time: 261.48308801651\n",
            "Epoch: 5, Steps: 2454 | Train Loss: 2505.8520865 Vali Loss: 204.1936493 Test Loss: 256.6670837\n",
            "EarlyStopping counter: 3 out of 3\n",
            "Early stopping\n",
            "test 22400\n",
            "test shape: (22400, 96, 49) (22400, 96, 49)\n",
            "mae:0.46653831005096436, mse:0.44363775849342346, rmse:0.6660614013671875, mape:2.1715087890625, mspe:25203.5625\n",
            "Hyperparameter Combination of FEDformer_train_on_custom_192:\n",
            "\n",
            "{'model': 'FEDformer', 'train_only': True, 'use_multi_gpu': False, 'use_gpu': True, 'learning_rate': 0.005, 'pred_len': 192, 'label_len': 48, 'seq_len': 96, 'use_amp': False, 'output_attention': False, 'features': 'M', 'checkpoints': './Checkpoints/FEDFormer_checkpoints', 'patience': 3, 'train_epochs': 6, 'data': 'custom', 'root_path': '/content/Transformer-based-solutions-for-the-long-term-time-series-forecasting/Datasets/CustomData/', 'data_path': 'my_.csv', 'target': 'System', 'freq': 'h', 'enc_in': 49, 'dec_in': 49, 'c_out': 49, 'factor': 5, 'd_model': 512, 'n_heads': 8, 'e_layers': 2, 'd_layers': 1, 'd_ff': 2048, 'dropout': 0.05, 'attn': 'prob', 'embed': 'timeF', 'activation': 'gelu', 'distil': True, 'mix': True, 'padding': 0, 'batch_size': 32, 'loss': 'mse', 'lradj': 'type1', 'num_workers': 0, 'des': 'exp', 'gpu': 0, 'modes': 32, 'moving_avg': [12], 'embed_type': 1}\n",
            "Use GPU: cuda:0\n",
            "train 78447\n",
            "val 11058\n",
            "test 22304\n",
            "\titers:  100, epoch: 1 | loss: 0.5420105\n",
            "\tspeed: 0.1074sec/iter | left time: 1568.8474sec\n",
            "\titers:  200, epoch: 1 | loss: 0.4217830\n",
            "\tspeed: 0.1082sec/iter | left time: 1569.4192sec\n",
            "\titers:  300, epoch: 1 | loss: 0.3540274\n",
            "\tspeed: 0.1077sec/iter | left time: 1551.5438sec\n",
            "\titers:  400, epoch: 1 | loss: 0.3371873\n",
            "\tspeed: 0.1079sec/iter | left time: 1543.9362sec\n",
            "\titers:  500, epoch: 1 | loss: 0.2980346\n",
            "\tspeed: 0.1080sec/iter | left time: 1534.0223sec\n",
            "\titers:  600, epoch: 1 | loss: 0.2978240\n",
            "\tspeed: 0.1077sec/iter | left time: 1519.4463sec\n",
            "\titers:  700, epoch: 1 | loss: 0.2975198\n",
            "\tspeed: 0.1074sec/iter | left time: 1504.8494sec\n",
            "\titers:  800, epoch: 1 | loss: 0.2638893\n",
            "\tspeed: 0.1074sec/iter | left time: 1493.5678sec\n",
            "\titers:  900, epoch: 1 | loss: 0.3173236\n",
            "\tspeed: 0.1075sec/iter | left time: 1484.3867sec\n",
            "\titers: 1000, epoch: 1 | loss: 0.3097058\n",
            "\tspeed: 0.1078sec/iter | left time: 1477.8012sec\n",
            "\titers: 1100, epoch: 1 | loss: 0.3257194\n",
            "\tspeed: 0.1078sec/iter | left time: 1467.3057sec\n",
            "\titers: 1200, epoch: 1 | loss: 0.2998177\n",
            "\tspeed: 0.1083sec/iter | left time: 1462.5579sec\n",
            "\titers: 1300, epoch: 1 | loss: 35812920.0000000\n",
            "\tspeed: 0.1082sec/iter | left time: 1450.9805sec\n",
            "\titers: 1400, epoch: 1 | loss: 5149678.0000000\n",
            "\tspeed: 0.1080sec/iter | left time: 1437.1361sec\n",
            "\titers: 1500, epoch: 1 | loss: 191584960.0000000\n",
            "\tspeed: 0.1079sec/iter | left time: 1424.7852sec\n",
            "\titers: 1600, epoch: 1 | loss: 11169720.0000000\n",
            "\tspeed: 0.1084sec/iter | left time: 1420.7217sec\n",
            "\titers: 1700, epoch: 1 | loss: 7676044.0000000\n",
            "\tspeed: 0.1083sec/iter | left time: 1408.8710sec\n",
            "\titers: 1800, epoch: 1 | loss: 6638803.0000000\n",
            "\tspeed: 0.1036sec/iter | left time: 1336.8954sec\n",
            "\titers: 1900, epoch: 1 | loss: 6953000.0000000\n",
            "\tspeed: 0.1039sec/iter | left time: 1330.2992sec\n",
            "\titers: 2000, epoch: 1 | loss: 6106786.0000000\n",
            "\tspeed: 0.1038sec/iter | left time: 1318.3823sec\n",
            "\titers: 2100, epoch: 1 | loss: 5251669.0000000\n",
            "\tspeed: 0.1031sec/iter | left time: 1299.9635sec\n",
            "\titers: 2200, epoch: 1 | loss: 4061486.5000000\n",
            "\tspeed: 0.1048sec/iter | left time: 1310.7790sec\n",
            "\titers: 2300, epoch: 1 | loss: 3680252.5000000\n",
            "\tspeed: 0.1051sec/iter | left time: 1303.6858sec\n",
            "\titers: 2400, epoch: 1 | loss: 3297714.5000000\n",
            "\tspeed: 0.1043sec/iter | left time: 1283.7753sec\n",
            "Epoch: 1 cost time: 261.6699743270874\n",
            "Epoch: 1, Steps: 2451 | Train Loss: 858383694.0655967 Vali Loss: 205590.1718750 Test Loss: 229989.1875000\n",
            ">>> Validation loss decreased (inf --> 205590.171875).  Saving model ...\n",
            "Updating learning rate to 0.005\n",
            "\titers:  100, epoch: 2 | loss: 3004593.2500000\n",
            "\tspeed: 0.4539sec/iter | left time: 5517.7431sec\n",
            "\titers:  200, epoch: 2 | loss: 3017418.5000000\n",
            "\tspeed: 0.1052sec/iter | left time: 1268.3314sec\n",
            "\titers:  300, epoch: 2 | loss: 2338892.5000000\n",
            "\tspeed: 0.1091sec/iter | left time: 1304.7392sec\n",
            "\titers:  400, epoch: 2 | loss: 2265363.5000000\n",
            "\tspeed: 0.1092sec/iter | left time: 1294.3938sec\n",
            "\titers:  500, epoch: 2 | loss: 2042580.8750000\n",
            "\tspeed: 0.1106sec/iter | left time: 1299.8044sec\n",
            "\titers:  600, epoch: 2 | loss: 1921371.1250000\n",
            "\tspeed: 0.1084sec/iter | left time: 1263.6405sec\n",
            "\titers:  700, epoch: 2 | loss: 1404838.1250000\n",
            "\tspeed: 0.1096sec/iter | left time: 1266.7541sec\n",
            "\titers:  800, epoch: 2 | loss: 1600348.2500000\n",
            "\tspeed: 0.1094sec/iter | left time: 1253.1428sec\n",
            "\titers:  900, epoch: 2 | loss: 1417530.5000000\n",
            "\tspeed: 0.1098sec/iter | left time: 1247.3588sec\n",
            "\titers: 1000, epoch: 2 | loss: 1376657.8750000\n",
            "\tspeed: 0.1096sec/iter | left time: 1233.6569sec\n",
            "\titers: 1100, epoch: 2 | loss: 1327193.2500000\n",
            "\tspeed: 0.1088sec/iter | left time: 1214.2525sec\n",
            "\titers: 1200, epoch: 2 | loss: 967531.4375000\n",
            "\tspeed: 0.1095sec/iter | left time: 1210.4175sec\n",
            "\titers: 1300, epoch: 2 | loss: 1060419.0000000\n",
            "\tspeed: 0.1100sec/iter | left time: 1205.1480sec\n",
            "\titers: 1400, epoch: 2 | loss: 891963.7500000\n",
            "\tspeed: 0.1094sec/iter | left time: 1187.9397sec\n",
            "\titers: 1500, epoch: 2 | loss: 832467.8125000\n",
            "\tspeed: 0.1104sec/iter | left time: 1187.5811sec\n",
            "\titers: 1600, epoch: 2 | loss: 737473.5000000\n",
            "\tspeed: 0.1094sec/iter | left time: 1165.3509sec\n",
            "\titers: 1700, epoch: 2 | loss: 734473.0000000\n",
            "\tspeed: 0.1080sec/iter | left time: 1139.5254sec\n",
            "\titers: 1800, epoch: 2 | loss: 699633.2500000\n",
            "\tspeed: 0.1098sec/iter | left time: 1147.7625sec\n",
            "\titers: 1900, epoch: 2 | loss: 726360.9375000\n",
            "\tspeed: 0.1077sec/iter | left time: 1115.5765sec\n",
            "\titers: 2000, epoch: 2 | loss: 583886.0625000\n",
            "\tspeed: 0.1092sec/iter | left time: 1120.0505sec\n",
            "\titers: 2100, epoch: 2 | loss: 573638.8125000\n",
            "\tspeed: 0.1099sec/iter | left time: 1116.1439sec\n",
            "\titers: 2200, epoch: 2 | loss: 579883.2500000\n",
            "\tspeed: 0.1104sec/iter | left time: 1110.5656sec\n",
            "\titers: 2300, epoch: 2 | loss: 496062.6875000\n",
            "\tspeed: 0.1104sec/iter | left time: 1099.0104sec\n",
            "\titers: 2400, epoch: 2 | loss: 464909.0937500\n",
            "\tspeed: 0.1114sec/iter | left time: 1097.7910sec\n",
            "Epoch: 2 cost time: 267.86344385147095\n",
            "Epoch: 2, Steps: 2451 | Train Loss: 1336731.7123113 Vali Loss: 22005.4238281 Test Loss: 27070.5800781\n",
            ">>> Validation loss decreased (205590.171875 --> 22005.423828).  Saving model ...\n",
            "Updating learning rate to 0.0025\n",
            "\titers:  100, epoch: 3 | loss: 418130.3437500\n",
            "\tspeed: 0.4662sec/iter | left time: 4524.2649sec\n",
            "\titers:  200, epoch: 3 | loss: 363630.7500000\n",
            "\tspeed: 0.1101sec/iter | left time: 1057.6513sec\n",
            "\titers:  300, epoch: 3 | loss: 377675.4062500\n",
            "\tspeed: 0.1088sec/iter | left time: 1033.7096sec\n",
            "\titers:  400, epoch: 3 | loss: 400311.2187500\n",
            "\tspeed: 0.1110sec/iter | left time: 1043.7166sec\n",
            "\titers:  500, epoch: 3 | loss: 384313.1250000\n",
            "\tspeed: 0.1076sec/iter | left time: 1001.2754sec\n",
            "\titers:  600, epoch: 3 | loss: 340163.6875000\n",
            "\tspeed: 0.1054sec/iter | left time: 970.0199sec\n",
            "\titers:  700, epoch: 3 | loss: 373403.3125000\n",
            "\tspeed: 0.1042sec/iter | left time: 948.6177sec\n",
            "\titers:  800, epoch: 3 | loss: 336510.3750000\n",
            "\tspeed: 0.1059sec/iter | left time: 953.1827sec\n",
            "\titers:  900, epoch: 3 | loss: 303558.0000000\n",
            "\tspeed: 0.1051sec/iter | left time: 935.6104sec\n",
            "\titers: 1000, epoch: 3 | loss: 296890.9687500\n",
            "\tspeed: 0.1066sec/iter | left time: 938.2448sec\n",
            "\titers: 1100, epoch: 3 | loss: 312491.6875000\n",
            "\tspeed: 0.1082sec/iter | left time: 942.1175sec\n",
            "\titers: 1200, epoch: 3 | loss: 269431.5625000\n",
            "\tspeed: 0.1084sec/iter | left time: 933.0233sec\n",
            "\titers: 1300, epoch: 3 | loss: 264673.2187500\n",
            "\tspeed: 0.1076sec/iter | left time: 915.0342sec\n",
            "\titers: 1400, epoch: 3 | loss: 267594.0937500\n",
            "\tspeed: 0.1082sec/iter | left time: 909.3306sec\n",
            "\titers: 1500, epoch: 3 | loss: 286762.0625000\n",
            "\tspeed: 0.1080sec/iter | left time: 897.1405sec\n",
            "\titers: 1600, epoch: 3 | loss: 248049.1718750\n",
            "\tspeed: 0.1082sec/iter | left time: 887.5423sec\n",
            "\titers: 1700, epoch: 3 | loss: 209889.9062500\n",
            "\tspeed: 0.1090sec/iter | left time: 883.0848sec\n",
            "\titers: 1800, epoch: 3 | loss: 227206.9531250\n",
            "\tspeed: 0.1081sec/iter | left time: 865.5294sec\n",
            "\titers: 1900, epoch: 3 | loss: 201107.0156250\n",
            "\tspeed: 0.1083sec/iter | left time: 856.0714sec\n",
            "\titers: 2000, epoch: 3 | loss: 195096.0625000\n",
            "\tspeed: 0.1085sec/iter | left time: 846.8976sec\n",
            "\titers: 2100, epoch: 3 | loss: 186624.7343750\n",
            "\tspeed: 0.1087sec/iter | left time: 837.6219sec\n",
            "\titers: 2200, epoch: 3 | loss: 204355.5000000\n",
            "\tspeed: 0.1083sec/iter | left time: 823.3999sec\n",
            "\titers: 2300, epoch: 3 | loss: 163336.2656250\n",
            "\tspeed: 0.1093sec/iter | left time: 820.4477sec\n",
            "\titers: 2400, epoch: 3 | loss: 145339.1562500\n",
            "\tspeed: 0.1088sec/iter | left time: 805.5340sec\n",
            "Epoch: 3 cost time: 264.70308923721313\n",
            "Epoch: 3, Steps: 2451 | Train Loss: 286032.3538097 Vali Loss: 7067.7275391 Test Loss: 9005.3906250\n",
            ">>> Validation loss decreased (22005.423828 --> 7067.727539).  Saving model ...\n",
            "Updating learning rate to 0.00125\n",
            "\titers:  100, epoch: 4 | loss: 139468.8750000\n",
            "\tspeed: 0.4586sec/iter | left time: 3327.0192sec\n",
            "\titers:  200, epoch: 4 | loss: 141984.3281250\n",
            "\tspeed: 0.1083sec/iter | left time: 774.8833sec\n",
            "\titers:  300, epoch: 4 | loss: 137521.6250000\n",
            "\tspeed: 0.1080sec/iter | left time: 761.6653sec\n",
            "\titers:  400, epoch: 4 | loss: 136319.0781250\n",
            "\tspeed: 0.1072sec/iter | left time: 745.6791sec\n",
            "\titers:  500, epoch: 4 | loss: 137205.4062500\n",
            "\tspeed: 0.1068sec/iter | left time: 732.0603sec\n",
            "\titers:  600, epoch: 4 | loss: 122648.2187500\n",
            "\tspeed: 0.1076sec/iter | left time: 727.0666sec\n",
            "\titers:  700, epoch: 4 | loss: 115485.8593750\n",
            "\tspeed: 0.1080sec/iter | left time: 718.6421sec\n",
            "\titers:  800, epoch: 4 | loss: 131721.2187500\n",
            "\tspeed: 0.1080sec/iter | left time: 708.1111sec\n",
            "\titers:  900, epoch: 4 | loss: 133202.1406250\n",
            "\tspeed: 0.1080sec/iter | left time: 697.2479sec\n",
            "\titers: 1000, epoch: 4 | loss: 115096.8125000\n",
            "\tspeed: 0.1040sec/iter | left time: 660.5287sec\n",
            "\titers: 1100, epoch: 4 | loss: 113865.4375000\n",
            "\tspeed: 0.1031sec/iter | left time: 644.8302sec\n",
            "\titers: 1200, epoch: 4 | loss: 111876.1328125\n",
            "\tspeed: 0.1034sec/iter | left time: 636.3313sec\n",
            "\titers: 1300, epoch: 4 | loss: 101151.3437500\n",
            "\tspeed: 0.1032sec/iter | left time: 624.5960sec\n",
            "\titers: 1400, epoch: 4 | loss: 111897.8515625\n",
            "\tspeed: 0.1064sec/iter | left time: 633.2648sec\n",
            "\titers: 1500, epoch: 4 | loss: 98057.9609375\n",
            "\tspeed: 0.1085sec/iter | left time: 635.1745sec\n",
            "\titers: 1600, epoch: 4 | loss: 96218.3281250\n",
            "\tspeed: 0.1088sec/iter | left time: 625.8828sec\n",
            "\titers: 1700, epoch: 4 | loss: 91613.9531250\n",
            "\tspeed: 0.1083sec/iter | left time: 612.6041sec\n",
            "\titers: 1800, epoch: 4 | loss: 85699.1015625\n",
            "\tspeed: 0.1084sec/iter | left time: 601.9160sec\n",
            "\titers: 1900, epoch: 4 | loss: 82717.2343750\n",
            "\tspeed: 0.1076sec/iter | left time: 586.7402sec\n",
            "\titers: 2000, epoch: 4 | loss: 85108.6640625\n",
            "\tspeed: 0.1076sec/iter | left time: 576.0652sec\n",
            "\titers: 2100, epoch: 4 | loss: 75838.6953125\n",
            "\tspeed: 0.1085sec/iter | left time: 570.2367sec\n",
            "\titers: 2200, epoch: 4 | loss: 73448.6796875\n",
            "\tspeed: 0.1078sec/iter | left time: 555.4140sec\n",
            "\titers: 2300, epoch: 4 | loss: 82676.5859375\n",
            "\tspeed: 0.1079sec/iter | left time: 545.5108sec\n",
            "\titers: 2400, epoch: 4 | loss: 81100.3671875\n",
            "\tspeed: 0.1081sec/iter | left time: 535.5376sec\n",
            "Epoch: 4 cost time: 262.7893795967102\n",
            "Epoch: 4, Steps: 2451 | Train Loss: 112777.0727987 Vali Loss: 3735.8796387 Test Loss: 4720.0751953\n",
            ">>> Validation loss decreased (7067.727539 --> 3735.879639).  Saving model ...\n",
            "Updating learning rate to 0.000625\n",
            "\titers:  100, epoch: 5 | loss: 68720.1406250\n",
            "\tspeed: 0.4613sec/iter | left time: 2215.5041sec\n",
            "\titers:  200, epoch: 5 | loss: 68855.6015625\n",
            "\tspeed: 0.1082sec/iter | left time: 508.7435sec\n",
            "\titers:  300, epoch: 5 | loss: 67174.0000000\n",
            "\tspeed: 0.1083sec/iter | left time: 498.3956sec\n",
            "\titers:  400, epoch: 5 | loss: 66121.0156250\n",
            "\tspeed: 0.1079sec/iter | left time: 485.9908sec\n",
            "\titers:  500, epoch: 5 | loss: 58131.0390625\n",
            "\tspeed: 0.1083sec/iter | left time: 476.7957sec\n",
            "\titers:  600, epoch: 5 | loss: 64384.6171875\n",
            "\tspeed: 0.1082sec/iter | left time: 465.7343sec\n",
            "\titers:  700, epoch: 5 | loss: 64123.3671875\n",
            "\tspeed: 0.1073sec/iter | left time: 451.1512sec\n",
            "\titers:  800, epoch: 5 | loss: 60905.4687500\n",
            "\tspeed: 0.1072sec/iter | left time: 439.9597sec\n",
            "\titers:  900, epoch: 5 | loss: 61034.7460938\n",
            "\tspeed: 0.1077sec/iter | left time: 431.2169sec\n",
            "\titers: 1000, epoch: 5 | loss: 57864.3203125\n",
            "\tspeed: 0.1080sec/iter | left time: 421.6557sec\n",
            "\titers: 1100, epoch: 5 | loss: 57825.6523438\n",
            "\tspeed: 0.1081sec/iter | left time: 411.0608sec\n",
            "\titers: 1200, epoch: 5 | loss: 56263.6210938\n",
            "\tspeed: 0.1077sec/iter | left time: 398.8914sec\n",
            "\titers: 1300, epoch: 5 | loss: 50065.9804688\n",
            "\tspeed: 0.1042sec/iter | left time: 375.3839sec\n",
            "\titers: 1400, epoch: 5 | loss: 53820.0039062\n",
            "\tspeed: 0.1053sec/iter | left time: 368.7334sec\n",
            "\titers: 1500, epoch: 5 | loss: 49799.0000000\n",
            "\tspeed: 0.1041sec/iter | left time: 354.0866sec\n",
            "\titers: 1600, epoch: 5 | loss: 49329.5234375\n",
            "\tspeed: 0.1030sec/iter | left time: 340.2407sec\n",
            "\titers: 1700, epoch: 5 | loss: 50244.9296875\n",
            "\tspeed: 0.1038sec/iter | left time: 332.6285sec\n",
            "\titers: 1800, epoch: 5 | loss: 49031.2382812\n",
            "\tspeed: 0.1087sec/iter | left time: 337.2095sec\n",
            "\titers: 1900, epoch: 5 | loss: 49546.6328125\n",
            "\tspeed: 0.1090sec/iter | left time: 327.3718sec\n",
            "\titers: 2000, epoch: 5 | loss: 44502.6796875\n",
            "\tspeed: 0.1096sec/iter | left time: 318.3065sec\n",
            "\titers: 2100, epoch: 5 | loss: 43369.9101562\n",
            "\tspeed: 0.1085sec/iter | left time: 304.2647sec\n",
            "\titers: 2200, epoch: 5 | loss: 39120.7500000\n",
            "\tspeed: 0.1090sec/iter | left time: 294.5139sec\n",
            "\titers: 2300, epoch: 5 | loss: 45554.5976562\n",
            "\tspeed: 0.1073sec/iter | left time: 279.2470sec\n",
            "\titers: 2400, epoch: 5 | loss: 45986.4882812\n",
            "\tspeed: 0.1072sec/iter | left time: 268.2852sec\n",
            "Epoch: 5 cost time: 262.99025082588196\n",
            "Epoch: 5, Steps: 2451 | Train Loss: 56086.1649614 Vali Loss: 2829.5021973 Test Loss: 3539.0869141\n",
            ">>> Validation loss decreased (3735.879639 --> 2829.502197).  Saving model ...\n",
            "Updating learning rate to 0.0003125\n",
            "\titers:  100, epoch: 6 | loss: 50126.4023438\n",
            "\tspeed: 0.4598sec/iter | left time: 1081.3860sec\n",
            "\titers:  200, epoch: 6 | loss: 42121.9453125\n",
            "\tspeed: 0.1093sec/iter | left time: 246.0424sec\n",
            "\titers:  300, epoch: 6 | loss: 42330.2226562\n",
            "\tspeed: 0.1091sec/iter | left time: 234.8441sec\n",
            "\titers:  400, epoch: 6 | loss: 41333.6992188\n",
            "\tspeed: 0.1091sec/iter | left time: 223.8144sec\n",
            "\titers:  500, epoch: 6 | loss: 38014.0273438\n",
            "\tspeed: 0.1094sec/iter | left time: 213.5510sec\n",
            "\titers:  600, epoch: 6 | loss: 40109.8281250\n",
            "\tspeed: 0.1086sec/iter | left time: 201.1478sec\n",
            "\titers:  700, epoch: 6 | loss: 39904.9414062\n",
            "\tspeed: 0.1089sec/iter | left time: 190.8735sec\n",
            "\titers:  800, epoch: 6 | loss: 42825.1093750\n",
            "\tspeed: 0.1089sec/iter | left time: 179.8872sec\n",
            "\titers:  900, epoch: 6 | loss: 37461.1289062\n",
            "\tspeed: 0.1096sec/iter | left time: 170.1373sec\n",
            "\titers: 1000, epoch: 6 | loss: 40291.2929688\n",
            "\tspeed: 0.1099sec/iter | left time: 159.5267sec\n",
            "\titers: 1100, epoch: 6 | loss: 37038.4531250\n",
            "\tspeed: 0.1090sec/iter | left time: 147.3518sec\n",
            "\titers: 1200, epoch: 6 | loss: 33676.0117188\n",
            "\tspeed: 0.1095sec/iter | left time: 137.0356sec\n",
            "\titers: 1300, epoch: 6 | loss: 35126.2578125\n",
            "\tspeed: 0.1111sec/iter | left time: 127.9719sec\n",
            "\titers: 1400, epoch: 6 | loss: 31120.6250000\n",
            "\tspeed: 0.1093sec/iter | left time: 115.0087sec\n",
            "\titers: 1500, epoch: 6 | loss: 30910.2890625\n",
            "\tspeed: 0.1098sec/iter | left time: 104.5158sec\n",
            "\titers: 1600, epoch: 6 | loss: 32354.2441406\n",
            "\tspeed: 0.1088sec/iter | left time: 92.6791sec\n",
            "\titers: 1700, epoch: 6 | loss: 28891.3867188\n",
            "\tspeed: 0.1101sec/iter | left time: 82.7834sec\n",
            "\titers: 1800, epoch: 6 | loss: 27818.3183594\n",
            "\tspeed: 0.1093sec/iter | left time: 71.2801sec\n",
            "\titers: 1900, epoch: 6 | loss: 27303.0546875\n",
            "\tspeed: 0.1094sec/iter | left time: 60.3997sec\n",
            "\titers: 2000, epoch: 6 | loss: 27113.1328125\n",
            "\tspeed: 0.1100sec/iter | left time: 49.7377sec\n",
            "\titers: 2100, epoch: 6 | loss: 25473.6601562\n",
            "\tspeed: 0.1093sec/iter | left time: 38.4745sec\n",
            "\titers: 2200, epoch: 6 | loss: 23975.2519531\n",
            "\tspeed: 0.1037sec/iter | left time: 26.1427sec\n",
            "\titers: 2300, epoch: 6 | loss: 25197.3808594\n",
            "\tspeed: 0.1037sec/iter | left time: 15.7627sec\n",
            "\titers: 2400, epoch: 6 | loss: 23066.9140625\n",
            "\tspeed: 0.1036sec/iter | left time: 5.3893sec\n",
            "Epoch: 6 cost time: 266.1324529647827\n",
            "Epoch: 6, Steps: 2451 | Train Loss: 34014.2891557 Vali Loss: 1778.2996826 Test Loss: 2080.4924316\n",
            ">>> Validation loss decreased (2829.502197 --> 1778.299683).  Saving model ...\n",
            "Updating learning rate to 0.00015625\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Model(\n",
              "  (decomp): series_decomp_multi(\n",
              "    (layer): Linear(in_features=1, out_features=1, bias=True)\n",
              "  )\n",
              "  (enc_embedding): DataEmbedding(\n",
              "    (value_embedding): TokenEmbedding(\n",
              "      (tokenConv): Conv1d(49, 512, kernel_size=(3,), stride=(1,), padding=(1,), bias=False, padding_mode=circular)\n",
              "    )\n",
              "    (position_embedding): PositionalEmbedding()\n",
              "    (temporal_embedding): TimeFeatureEmbedding(\n",
              "      (embed): Linear(in_features=4, out_features=512, bias=False)\n",
              "    )\n",
              "    (dropout): Dropout(p=0.05, inplace=False)\n",
              "  )\n",
              "  (dec_embedding): DataEmbedding(\n",
              "    (value_embedding): TokenEmbedding(\n",
              "      (tokenConv): Conv1d(49, 512, kernel_size=(3,), stride=(1,), padding=(1,), bias=False, padding_mode=circular)\n",
              "    )\n",
              "    (position_embedding): PositionalEmbedding()\n",
              "    (temporal_embedding): TimeFeatureEmbedding(\n",
              "      (embed): Linear(in_features=4, out_features=512, bias=False)\n",
              "    )\n",
              "    (dropout): Dropout(p=0.05, inplace=False)\n",
              "  )\n",
              "  (encoder): Encoder(\n",
              "    (attn_layers): ModuleList(\n",
              "      (0): EncoderLayer(\n",
              "        (attention): AutoCorrelationLayer(\n",
              "          (inner_correlation): FourierBlock()\n",
              "          (query_projection): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (key_projection): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (value_projection): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (out_projection): Linear(in_features=512, out_features=512, bias=True)\n",
              "        )\n",
              "        (conv1): Conv1d(512, 2048, kernel_size=(1,), stride=(1,), bias=False)\n",
              "        (conv2): Conv1d(2048, 512, kernel_size=(1,), stride=(1,), bias=False)\n",
              "        (decomp1): series_decomp_multi(\n",
              "          (layer): Linear(in_features=1, out_features=1, bias=True)\n",
              "        )\n",
              "        (decomp2): series_decomp_multi(\n",
              "          (layer): Linear(in_features=1, out_features=1, bias=True)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.05, inplace=False)\n",
              "      )\n",
              "      (1): EncoderLayer(\n",
              "        (attention): AutoCorrelationLayer(\n",
              "          (inner_correlation): FourierBlock()\n",
              "          (query_projection): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (key_projection): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (value_projection): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (out_projection): Linear(in_features=512, out_features=512, bias=True)\n",
              "        )\n",
              "        (conv1): Conv1d(512, 2048, kernel_size=(1,), stride=(1,), bias=False)\n",
              "        (conv2): Conv1d(2048, 512, kernel_size=(1,), stride=(1,), bias=False)\n",
              "        (decomp1): series_decomp_multi(\n",
              "          (layer): Linear(in_features=1, out_features=1, bias=True)\n",
              "        )\n",
              "        (decomp2): series_decomp_multi(\n",
              "          (layer): Linear(in_features=1, out_features=1, bias=True)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.05, inplace=False)\n",
              "      )\n",
              "    )\n",
              "    (norm): my_Layernorm(\n",
              "      (layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "  )\n",
              "  (decoder): Decoder(\n",
              "    (layers): ModuleList(\n",
              "      (0): DecoderLayer(\n",
              "        (self_attention): AutoCorrelationLayer(\n",
              "          (inner_correlation): FourierBlock()\n",
              "          (query_projection): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (key_projection): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (value_projection): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (out_projection): Linear(in_features=512, out_features=512, bias=True)\n",
              "        )\n",
              "        (cross_attention): AutoCorrelationLayer(\n",
              "          (inner_correlation): FourierCrossAttention()\n",
              "          (query_projection): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (key_projection): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (value_projection): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (out_projection): Linear(in_features=512, out_features=512, bias=True)\n",
              "        )\n",
              "        (conv1): Conv1d(512, 2048, kernel_size=(1,), stride=(1,), bias=False)\n",
              "        (conv2): Conv1d(2048, 512, kernel_size=(1,), stride=(1,), bias=False)\n",
              "        (decomp1): series_decomp_multi(\n",
              "          (layer): Linear(in_features=1, out_features=1, bias=True)\n",
              "        )\n",
              "        (decomp2): series_decomp_multi(\n",
              "          (layer): Linear(in_features=1, out_features=1, bias=True)\n",
              "        )\n",
              "        (decomp3): series_decomp_multi(\n",
              "          (layer): Linear(in_features=1, out_features=1, bias=True)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.05, inplace=False)\n",
              "        (projection): Conv1d(512, 49, kernel_size=(3,), stride=(1,), padding=(1,), bias=False, padding_mode=circular)\n",
              "      )\n",
              "    )\n",
              "    (norm): my_Layernorm(\n",
              "      (layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "    (projection): Linear(in_features=512, out_features=49, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "exp.test(setting)\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "IFwqA5dObe1L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Informer"
      ],
      "metadata": {
        "id": "mcEs0hEqc-fj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "# Change directory to the Transformer-based solutions path\n",
        "os.chdir('/content/Transformer-based-solutions-for-the-long-term-time-series-forecasting/Datasets/CustomData')\n",
        "\n",
        "# Ensure the Transformer-based solutions path is in sys.path\n",
        "if 'Transformer-based-solutions-for-the-long-term-time-series-forecasting' not in sys.path:\n",
        "    sys.path += ['Transformer-based-solutions-for-the-long-term-time-series-forecasting']\n",
        "\n",
        "from exp.exp_Informer import Exp_Informer\n",
        "\n",
        "# Define the dotdict class\n",
        "class dotdict(dict):\n",
        "    \"\"\"dot.notation access to dictionary attributes\"\"\"\n",
        "    __getattr__ = dict.get\n",
        "    __setattr__ = dict.__setitem__\n",
        "    __delattr__ = dict.__delitem__\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "fix_seed = 2021\n",
        "random.seed(fix_seed)\n",
        "torch.manual_seed(fix_seed)\n",
        "np.random.seed(fix_seed)\n",
        "\n",
        "# Define the hyperparameters in the dotdict format\n",
        "args = dotdict()\n",
        "\n",
        "# Device Hyperparameters\n",
        "args.use_multi_gpu = False\n",
        "args.num_workers = 0\n",
        "args.use_gpu = torch.cuda.is_available()\n",
        "args.gpu = 0  # The index of the GPU to use\n",
        "\n",
        "# Dataset Hyperparameters\n",
        "args.data = 'custom'  # dataset name\n",
        "args.root_path = '/content/Transformer-based-solutions-for-the-long-term-time-series-forecasting/Datasets/CustomData'  # root path of data file\n",
        "args.data_path = 'my_.csv'  # data file\n",
        "args.features = 'M'  # forecasting task\n",
        "args.target = 'System'  # target feature\n",
        "args.freq = 'h'  # frequency for time features encoding\n",
        "args.embed = 'timeF'  # time features encoding\n",
        "args.padding = 0  # the amount of padding to add\n",
        "\n",
        "# Experiment Hyperparameters\n",
        "args.output_attention = False  # whether to output attention in encoder\n",
        "args.use_amp = False  # whether to use automatic mixed precision training\n",
        "args.train_only = True\n",
        "args.train_epochs = 8  # number of epochs to train\n",
        "args.batch_size = 32  # batch size\n",
        "args.learning_rate = 0.0001  # learning rate\n",
        "args.lradj = 'type1'  # learning rate adjustment strategy\n",
        "args.loss = 'mse'  # loss function\n",
        "args.patience = 3  # early stopping patience\n",
        "args.des = 'test'  # description of the experiment\n",
        "args.itr = 1  # iteration of the experiment\n",
        "\n",
        "# Model Hyperparameters\n",
        "args.model = 'informer'\n",
        "args.checkpoints = './Checkpoints/Informer_checkpoints'  # location of model checkpoints\n",
        "\n",
        "# ProbSparse Self-attention Hyperparameters\n",
        "args.attn = 'prob'  # attention used in encoder\n",
        "args.d_model = 512  # dimension of model\n",
        "args.n_heads = 16  # number of attention heads\n",
        "args.factor = 5  # probsparse attn factor\n",
        "args.dropout = 0.1  # dropout\n",
        "args.d_ff = 2048  # dimension of feedforward network\n",
        "args.activation = 'gelu'  # activation function\n",
        "args.mix = True  # apply a linear projection to the concatenated outputs of the attention heads\n",
        "\n",
        "# Sequence Lengths and Embedding Types\n",
        "args.enc_in = 49  # encoder input size\n",
        "args.dec_in = 49  # decoder input size\n",
        "args.c_out = 49  # output size\n",
        "args.e_layers = 2  # number of encoder layers\n",
        "args.d_layers = 1  # number of decoder layers\n",
        "args.seq_len = 336  # input sequence length\n",
        "args.label_len = 48  # start token length\n",
        "args.pred_len = 96  # prediction sequence length\n",
        "args.distil = True  # use distilled architecture\n",
        "args.embed_type = 1  # embedding type\n",
        "\n",
        "# Adjusting the learning rate correctly\n",
        "def adjust_learning_rate(optimizer, epoch, args):\n",
        "    if args.lradj == 'type1':\n",
        "        lr_adjust = {\n",
        "            2: 5e-5, 4: 1e-5, 6: 5e-6, 8: 1e-6,\n",
        "            10: 5e-7, 15: 1e-7, 20: 5e-8\n",
        "        }\n",
        "    elif args.lradj == 'type2':\n",
        "        lr_adjust = {epoch: args.learning_rate * (0.5 ** ((epoch - 1) // 1))}\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown lr adjustment type: {args.lradj}\")\n",
        "\n",
        "    if epoch in lr_adjust:\n",
        "        lr = lr_adjust[epoch]\n",
        "        for param_group in optimizer.param_groups:\n",
        "            param_group['lr'] = lr\n",
        "        print(f\"Updating learning rate to {lr}\")\n",
        "    else:\n",
        "        print(f\"No learning rate adjustment for epoch {epoch}\")\n",
        "\n",
        "# Define the experiment and settings\n",
        "setting = f'{args.model}_train_on_{args.data}_{args.pred_len}_test'\n",
        "print(f\"Hyperparameter Combination of {setting}:\\n\")\n",
        "print(args)\n",
        "\n",
        "# Initialize and train the model\n",
        "Exp = Exp_Informer\n",
        "exp = Exp(args)\n",
        "exp.train(setting)\n",
        "\n",
        "# Test the model\n",
        "exp.test(setting)\n",
        "\n",
        "# Clear GPU cache\n",
        "torch.cuda.empty_cache()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AfkTkvMxc_15",
        "outputId": "0ce8d27b-2a15-4e47-c2a1-f52749a39194"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hyperparameter Combination of informer_train_on_custom_96_test:\n",
            "\n",
            "{'use_multi_gpu': False, 'num_workers': 0, 'use_gpu': True, 'gpu': 0, 'data': 'custom', 'root_path': '/content/Transformer-based-solutions-for-the-long-term-time-series-forecasting/Datasets/CustomData', 'data_path': 'my_.csv', 'features': 'M', 'target': 'System', 'freq': 'h', 'embed': 'timeF', 'padding': 0, 'output_attention': False, 'use_amp': False, 'train_only': True, 'train_epochs': 8, 'batch_size': 32, 'learning_rate': 0.0001, 'lradj': 'type1', 'loss': 'mse', 'patience': 3, 'des': 'test', 'itr': 1, 'model': 'informer', 'checkpoints': './Checkpoints/Informer_checkpoints', 'attn': 'prob', 'd_model': 512, 'n_heads': 16, 'factor': 5, 'dropout': 0.1, 'd_ff': 2048, 'activation': 'gelu', 'mix': True, 'enc_in': 49, 'dec_in': 49, 'c_out': 49, 'e_layers': 2, 'd_layers': 1, 'seq_len': 336, 'label_len': 48, 'pred_len': 96, 'distil': True, 'embed_type': 1}\n",
            "Use GPU: cuda:0\n",
            "train 78303\n",
            "val 11154\n",
            "test 22400\n",
            "\titers:  100, epoch: 1 | loss: 0.4706820\n",
            "\tspeed: 0.0532sec/iter | left time: 1035.4271sec\n",
            "\titers:  200, epoch: 1 | loss: 0.4490502\n",
            "\tspeed: 0.0531sec/iter | left time: 1028.2252sec\n",
            "\titers:  300, epoch: 1 | loss: 0.2972274\n",
            "\tspeed: 0.0535sec/iter | left time: 1030.1805sec\n",
            "\titers:  400, epoch: 1 | loss: 0.2526431\n",
            "\tspeed: 0.0532sec/iter | left time: 1019.2955sec\n",
            "\titers:  500, epoch: 1 | loss: 0.2337902\n",
            "\tspeed: 0.0540sec/iter | left time: 1030.1842sec\n",
            "\titers:  600, epoch: 1 | loss: 0.2329762\n",
            "\tspeed: 0.0528sec/iter | left time: 1002.2015sec\n",
            "\titers:  700, epoch: 1 | loss: 0.2232374\n",
            "\tspeed: 0.0528sec/iter | left time: 996.3908sec\n",
            "\titers:  800, epoch: 1 | loss: 0.2224289\n",
            "\tspeed: 0.0531sec/iter | left time: 995.7924sec\n",
            "\titers:  900, epoch: 1 | loss: 0.1998422\n",
            "\tspeed: 0.0529sec/iter | left time: 986.6829sec\n",
            "\titers: 1000, epoch: 1 | loss: 0.1915130\n",
            "\tspeed: 0.0526sec/iter | left time: 976.0459sec\n",
            "\titers: 1100, epoch: 1 | loss: 0.2137838\n",
            "\tspeed: 0.0523sec/iter | left time: 966.5951sec\n",
            "\titers: 1200, epoch: 1 | loss: 0.1896340\n",
            "\tspeed: 0.0534sec/iter | left time: 979.9989sec\n",
            "\titers: 1300, epoch: 1 | loss: 0.2081843\n",
            "\tspeed: 0.0533sec/iter | left time: 973.0471sec\n",
            "\titers: 1400, epoch: 1 | loss: 0.2059464\n",
            "\tspeed: 0.0536sec/iter | left time: 974.1535sec\n",
            "\titers: 1500, epoch: 1 | loss: 0.2040235\n",
            "\tspeed: 0.0531sec/iter | left time: 959.0845sec\n",
            "\titers: 1600, epoch: 1 | loss: 0.1803854\n",
            "\tspeed: 0.0542sec/iter | left time: 973.2232sec\n",
            "\titers: 1700, epoch: 1 | loss: 0.1827094\n",
            "\tspeed: 0.0525sec/iter | left time: 938.0269sec\n",
            "\titers: 1800, epoch: 1 | loss: 0.1641334\n",
            "\tspeed: 0.0533sec/iter | left time: 946.3613sec\n",
            "\titers: 1900, epoch: 1 | loss: 0.1580168\n",
            "\tspeed: 0.0524sec/iter | left time: 925.7190sec\n",
            "\titers: 2000, epoch: 1 | loss: 0.1582351\n",
            "\tspeed: 0.0530sec/iter | left time: 931.5833sec\n",
            "\titers: 2100, epoch: 1 | loss: 0.1588739\n",
            "\tspeed: 0.0534sec/iter | left time: 932.7956sec\n",
            "\titers: 2200, epoch: 1 | loss: 0.1699164\n",
            "\tspeed: 0.0522sec/iter | left time: 906.3387sec\n",
            "\titers: 2300, epoch: 1 | loss: 0.1439722\n",
            "\tspeed: 0.0538sec/iter | left time: 929.7081sec\n",
            "\titers: 2400, epoch: 1 | loss: 0.1656423\n",
            "\tspeed: 0.0524sec/iter | left time: 899.3784sec\n",
            "Epoch: 1 cost time: 129.84125781059265\n",
            "Epoch: 1, Steps: 2446 | Train Loss: 0.2253453 Vali Loss: 1.6456850 Test Loss: 2.2553604\n",
            ">>> Validation loss decreased (inf --> 1.645685).  Saving model ...\n",
            "Updating learning rate to 0.0001\n",
            "\titers:  100, epoch: 2 | loss: 0.1461192\n",
            "\tspeed: 0.3080sec/iter | left time: 5243.6791sec\n",
            "\titers:  200, epoch: 2 | loss: 0.1455293\n",
            "\tspeed: 0.0531sec/iter | left time: 898.7711sec\n",
            "\titers:  300, epoch: 2 | loss: 0.1338575\n",
            "\tspeed: 0.0539sec/iter | left time: 906.6260sec\n",
            "\titers:  400, epoch: 2 | loss: 0.1373553\n",
            "\tspeed: 0.0523sec/iter | left time: 874.1064sec\n",
            "\titers:  500, epoch: 2 | loss: 0.1364730\n",
            "\tspeed: 0.0531sec/iter | left time: 882.3546sec\n",
            "\titers:  600, epoch: 2 | loss: 0.1433825\n",
            "\tspeed: 0.0525sec/iter | left time: 867.9740sec\n",
            "\titers:  700, epoch: 2 | loss: 0.1278049\n",
            "\tspeed: 0.0532sec/iter | left time: 874.1763sec\n",
            "\titers:  800, epoch: 2 | loss: 0.1307939\n",
            "\tspeed: 0.0536sec/iter | left time: 875.4067sec\n",
            "\titers:  900, epoch: 2 | loss: 0.1271092\n",
            "\tspeed: 0.0531sec/iter | left time: 862.1871sec\n",
            "\titers: 1000, epoch: 2 | loss: 0.1313736\n",
            "\tspeed: 0.0532sec/iter | left time: 857.3739sec\n",
            "\titers: 1100, epoch: 2 | loss: 0.1235397\n",
            "\tspeed: 0.0529sec/iter | left time: 847.9605sec\n",
            "\titers: 1200, epoch: 2 | loss: 0.1226983\n",
            "\tspeed: 0.0532sec/iter | left time: 847.2391sec\n",
            "\titers: 1300, epoch: 2 | loss: 0.1252319\n",
            "\tspeed: 0.0535sec/iter | left time: 847.2020sec\n",
            "\titers: 1400, epoch: 2 | loss: 0.1250332\n",
            "\tspeed: 0.0532sec/iter | left time: 835.7646sec\n",
            "\titers: 1500, epoch: 2 | loss: 0.1245395\n",
            "\tspeed: 0.0521sec/iter | left time: 814.4569sec\n",
            "\titers: 1600, epoch: 2 | loss: 0.1206096\n",
            "\tspeed: 0.0534sec/iter | left time: 828.9095sec\n",
            "\titers: 1700, epoch: 2 | loss: 0.1232753\n",
            "\tspeed: 0.0535sec/iter | left time: 824.4055sec\n",
            "\titers: 1800, epoch: 2 | loss: 0.1078621\n",
            "\tspeed: 0.0534sec/iter | left time: 818.4032sec\n",
            "\titers: 1900, epoch: 2 | loss: 0.1110131\n",
            "\tspeed: 0.0535sec/iter | left time: 814.2002sec\n",
            "\titers: 2000, epoch: 2 | loss: 0.1135092\n",
            "\tspeed: 0.0530sec/iter | left time: 801.5968sec\n",
            "\titers: 2100, epoch: 2 | loss: 0.1173839\n",
            "\tspeed: 0.0522sec/iter | left time: 783.5838sec\n",
            "\titers: 2200, epoch: 2 | loss: 0.1118145\n",
            "\tspeed: 0.0520sec/iter | left time: 775.4710sec\n",
            "\titers: 2300, epoch: 2 | loss: 0.1050753\n",
            "\tspeed: 0.0524sec/iter | left time: 776.0744sec\n",
            "\titers: 2400, epoch: 2 | loss: 0.1014945\n",
            "\tspeed: 0.0516sec/iter | left time: 759.3912sec\n",
            "Epoch: 2 cost time: 129.4602189064026\n",
            "Epoch: 2, Steps: 2446 | Train Loss: 0.1252194 Vali Loss: 2.2678244 Test Loss: 3.0794115\n",
            "EarlyStopping counter: 1 out of 3\n",
            "Updating learning rate to 5e-05\n",
            "\titers:  100, epoch: 3 | loss: 0.0968596\n",
            "\tspeed: 0.3049sec/iter | left time: 4444.0869sec\n",
            "\titers:  200, epoch: 3 | loss: 0.0950832\n",
            "\tspeed: 0.0533sec/iter | left time: 772.2094sec\n",
            "\titers:  300, epoch: 3 | loss: 0.0951347\n",
            "\tspeed: 0.0533sec/iter | left time: 766.5213sec\n",
            "\titers:  400, epoch: 3 | loss: 0.0946404\n",
            "\tspeed: 0.0538sec/iter | left time: 768.3123sec\n",
            "\titers:  500, epoch: 3 | loss: 0.0953953\n",
            "\tspeed: 0.0537sec/iter | left time: 761.9383sec\n",
            "\titers:  600, epoch: 3 | loss: 0.0911601\n",
            "\tspeed: 0.0535sec/iter | left time: 752.4541sec\n",
            "\titers:  700, epoch: 3 | loss: 0.0949390\n",
            "\tspeed: 0.0525sec/iter | left time: 733.2266sec\n",
            "\titers:  800, epoch: 3 | loss: 0.0965994\n",
            "\tspeed: 0.0535sec/iter | left time: 742.6477sec\n",
            "\titers:  900, epoch: 3 | loss: 0.0937300\n",
            "\tspeed: 0.0531sec/iter | left time: 732.0775sec\n",
            "\titers: 1000, epoch: 3 | loss: 0.0942879\n",
            "\tspeed: 0.0537sec/iter | left time: 734.0027sec\n",
            "\titers: 1100, epoch: 3 | loss: 0.0894616\n",
            "\tspeed: 0.0540sec/iter | left time: 733.1712sec\n",
            "\titers: 1200, epoch: 3 | loss: 0.0893776\n",
            "\tspeed: 0.0536sec/iter | left time: 722.1651sec\n",
            "\titers: 1300, epoch: 3 | loss: 0.0911271\n",
            "\tspeed: 0.0532sec/iter | left time: 712.1963sec\n",
            "\titers: 1400, epoch: 3 | loss: 0.0911405\n",
            "\tspeed: 0.0528sec/iter | left time: 700.5842sec\n",
            "\titers: 1500, epoch: 3 | loss: 0.0837610\n",
            "\tspeed: 0.0531sec/iter | left time: 699.4512sec\n",
            "\titers: 1600, epoch: 3 | loss: 0.0892394\n",
            "\tspeed: 0.0533sec/iter | left time: 696.9858sec\n",
            "\titers: 1700, epoch: 3 | loss: 0.0860211\n",
            "\tspeed: 0.0540sec/iter | left time: 700.7861sec\n",
            "\titers: 1800, epoch: 3 | loss: 0.0884896\n",
            "\tspeed: 0.0536sec/iter | left time: 690.3628sec\n",
            "\titers: 1900, epoch: 3 | loss: 0.0851557\n",
            "\tspeed: 0.0533sec/iter | left time: 681.4162sec\n",
            "\titers: 2000, epoch: 3 | loss: 0.0859143\n",
            "\tspeed: 0.0534sec/iter | left time: 676.3339sec\n",
            "\titers: 2100, epoch: 3 | loss: 0.0858584\n",
            "\tspeed: 0.0535sec/iter | left time: 672.9895sec\n",
            "\titers: 2200, epoch: 3 | loss: 0.0858744\n",
            "\tspeed: 0.0531sec/iter | left time: 662.3929sec\n",
            "\titers: 2300, epoch: 3 | loss: 0.0825029\n",
            "\tspeed: 0.0537sec/iter | left time: 665.0325sec\n",
            "\titers: 2400, epoch: 3 | loss: 0.0863232\n",
            "\tspeed: 0.0534sec/iter | left time: 655.4915sec\n",
            "Epoch: 3 cost time: 130.70774674415588\n",
            "Epoch: 3, Steps: 2446 | Train Loss: 0.0914275 Vali Loss: 2.4048977 Test Loss: 3.2128627\n",
            "EarlyStopping counter: 2 out of 3\n",
            "Updating learning rate to 2.5e-05\n",
            "\titers:  100, epoch: 4 | loss: 0.0786151\n",
            "\tspeed: 0.3097sec/iter | left time: 3756.6727sec\n",
            "\titers:  200, epoch: 4 | loss: 0.0819946\n",
            "\tspeed: 0.0539sec/iter | left time: 648.1589sec\n",
            "\titers:  300, epoch: 4 | loss: 0.0831884\n",
            "\tspeed: 0.0533sec/iter | left time: 636.4674sec\n",
            "\titers:  400, epoch: 4 | loss: 0.0844937\n",
            "\tspeed: 0.0535sec/iter | left time: 632.4018sec\n",
            "\titers:  500, epoch: 4 | loss: 0.0778908\n",
            "\tspeed: 0.0535sec/iter | left time: 627.9786sec\n",
            "\titers:  600, epoch: 4 | loss: 0.0795475\n",
            "\tspeed: 0.0538sec/iter | left time: 625.7919sec\n",
            "\titers:  700, epoch: 4 | loss: 0.0793113\n",
            "\tspeed: 0.0536sec/iter | left time: 617.6293sec\n",
            "\titers:  800, epoch: 4 | loss: 0.0821473\n",
            "\tspeed: 0.0531sec/iter | left time: 606.9107sec\n",
            "\titers:  900, epoch: 4 | loss: 0.0812152\n",
            "\tspeed: 0.0537sec/iter | left time: 608.0445sec\n",
            "\titers: 1000, epoch: 4 | loss: 0.0754165\n",
            "\tspeed: 0.0528sec/iter | left time: 592.8604sec\n",
            "\titers: 1100, epoch: 4 | loss: 0.0824416\n",
            "\tspeed: 0.0542sec/iter | left time: 603.1300sec\n",
            "\titers: 1200, epoch: 4 | loss: 0.0773252\n",
            "\tspeed: 0.0537sec/iter | left time: 591.9973sec\n",
            "\titers: 1300, epoch: 4 | loss: 0.0749850\n",
            "\tspeed: 0.0535sec/iter | left time: 584.2747sec\n",
            "\titers: 1400, epoch: 4 | loss: 0.0760546\n",
            "\tspeed: 0.0538sec/iter | left time: 582.7905sec\n",
            "\titers: 1500, epoch: 4 | loss: 0.0684308\n",
            "\tspeed: 0.0536sec/iter | left time: 575.3336sec\n",
            "\titers: 1600, epoch: 4 | loss: 0.0762178\n",
            "\tspeed: 0.0538sec/iter | left time: 571.8939sec\n",
            "\titers: 1700, epoch: 4 | loss: 0.0794315\n",
            "\tspeed: 0.0539sec/iter | left time: 567.6282sec\n",
            "\titers: 1800, epoch: 4 | loss: 0.0727090\n",
            "\tspeed: 0.0532sec/iter | left time: 554.8268sec\n",
            "\titers: 1900, epoch: 4 | loss: 0.0723202\n",
            "\tspeed: 0.0536sec/iter | left time: 553.4896sec\n",
            "\titers: 2000, epoch: 4 | loss: 0.0752619\n",
            "\tspeed: 0.0543sec/iter | left time: 555.4765sec\n",
            "\titers: 2100, epoch: 4 | loss: 0.0750624\n",
            "\tspeed: 0.0529sec/iter | left time: 535.9810sec\n",
            "\titers: 2200, epoch: 4 | loss: 0.0799370\n",
            "\tspeed: 0.0542sec/iter | left time: 543.2929sec\n",
            "\titers: 2300, epoch: 4 | loss: 0.0808881\n",
            "\tspeed: 0.0540sec/iter | left time: 536.2057sec\n",
            "\titers: 2400, epoch: 4 | loss: 0.0762090\n",
            "\tspeed: 0.0539sec/iter | left time: 529.6194sec\n",
            "Epoch: 4 cost time: 131.17947340011597\n",
            "Epoch: 4, Steps: 2446 | Train Loss: 0.0782562 Vali Loss: 2.4368436 Test Loss: 3.2799644\n",
            "EarlyStopping counter: 3 out of 3\n",
            "Early stopping\n",
            "test 22400\n",
            "test shape: (22400, 96, 49) (22400, 96, 49)\n",
            "mae:1.194594144821167, mse:2.255303144454956, rmse:1.5017666816711426, mape:3.4171738624572754, mspe:36864.89453125\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eg9cuqoPdAMU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}